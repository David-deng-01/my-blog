---
title: 2023-08-31 学习笔记
layout: post
tags:
  - 笔记
  - Pytorch
categories:
  - 笔记
  - Pytorch
lang: zh-CN
abbrlink: 677
date: 2023-08-31 23:12:39
---

# 2023-08-31 学习笔记

## 1. 预训练语言模型的使用

### 问题：

`如何使用预训练的语言模型在已有的数据集上进行微调？`

### 解决方案如下：

1. 首先下载一个预训练语言模型，这里使用的是 `cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual` 可以直接在[Hugging Face](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual) 上下载
2. 预处理数据集(很关键！！)，数据集如果没有处理好，模型运行过程中容易报错，例如 `3. 错误解决` 中就出现了因为数据集没有处理好而导致的错误
3. 加载模型和分词器
4. 模型训练
5. 模型评估

### 代码如下：

1. 项目配置文件

   ```yaml
   # hydra 配置
   defaults:
     - _self_ # 优先级最高, 会覆盖其他配置
     - override hydra/hydra_logging: disabled # 禁用hydra日志, 会覆盖其他配置
     - override hydra/job_logging: disabled # 禁用hydra日志, 会覆盖其他配置

   hydra:
     output_subdir: null # 输出目录
     run: # 运行配置
       dir: . # 运行目录，相对路径

   # 项目配置
   project:
     name: Mongolian Sentiment Classification # 项目名称
     version: 0.0.1 # 项目版本

   resume: "output/epoch_4_f1_0.2792.pth" # 恢复训练模型路径
   lr: 1e-5 # 学习率
   train_batch_size: 64 # 训练batch size
   valid_batch_size: 128 # 验证batch size
   warm_up_steps: 500 # warm up steps
   epochs: 10 # 训练轮数
   device: "cuda:2" # 训练设备

   pretrained_model_name_or_path: "model" # 预训练模型路径
   output_dir: "output" # 输出目录
   train_file_path: "data/train.csv" # 训练数据路径
   valid_file_path: "data/valid.csv" # 验证数据路径
   weight_decay: 0.01 # 权重衰减

   plm_config:
     max_length: 256 # 最大序列长度
     num_labels: 6
     label2id:
       neural: 0
       happy: 1
       angry: 2
       sad: 3
       fear: 4
       surprise: 5
     id2label:
       0: neural
       1: happy
       2: angry
       3: sad
       4: fear
       5: surprise
   ```

2. 数据预处理代码：

   ```python
   def split_dataset(data_path: str = "data/data.csv", ratio=0.8):
       """
           划分数据集
           :param ratio: 划分比例
           :param data_path: 数据集路径
           :return:
       """
       # 读取文件
       data = pd.read_csv(data_path, encoding='GBK', names=['text', 'label'])
       data.dropna(axis=0, inplace=True)  # 删除空行
       data.dropna(axis=1, inplace=True)  # 删除空行

       # 划分数据集
       train_data = data.sample(frac=ratio, random_state=0, axis=0)
       valid_data = data.drop(train_data.index)

       print(f"数据集大小：{data.shape[0]}")
       print(f"训练集大小：{train_data.shape[0]}")
       print(f"验证集大小：{valid_data.shape[0]}")

       # 保存数据集
       train_data.to_csv("data/train.csv", columns=["text", "label"], index=False, encoding='GBK')
       valid_data.to_csv("data/valid.csv", columns=["text", "label"], index=False, encoding='GBK')

   ```

3. 加载模型和分词器

   ```python
   def load_pretrained_model(config):
       """
           加载预训练语言模型
           :param config: 配置文件
           :return: 模型和分词器
       """
       # 加载配置文件和修改配置文件
       plm_config = XLMRobertaConfig.from_pretrained(config['pretrained_model_name_or_path'])
       for key, value in config['plm_config'].items():
           setattr(plm_config, key, value)

       # 加载预训练语言模型和分词器
       model = XLMRobertaForSequenceClassification.from_pretrained(config['pretrained_model_name_or_path'])
       tokenizer = XLMRobertaTokenizerFast.from_pretrained(config['pretrained_model_name_or_path'])

       # 修改模型的分类器配置文件
       model.plm_config = plm_config
       model.num_labels = plm_config.num_labels
       model.classifier = RobertaClassificationHead(plm_config)

       # 冻结模型
       freeze_model(model.roberta)

       # 返回模型和分词器
       return model, tokenizer

   ```

4. 模型训练

   ```python
   def do_train(config):
       epochs, device = config['epochs'], torch.device(config['device'])

       # 加载数据集
       train_loader, valid_loader = load_data(config)

       # 计算总步数
       total_steps = len(train_loader) * epochs

       # 加载模型
       model, tokenizer = load_pretrained_model(config)
       model.to(device)

       # 加载优化器
       optimizer = AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])
       scheduler = get_linear_schedule_with_warmup(
           optimizer=optimizer,
           num_warmup_steps=config['warm_up_steps'],
           num_training_steps=total_steps
       )

       best_ckpt_path, best_f1 = "", 0

       # 断续训练
       if config['resume']:
           assert os.path.exists(config['resume']), f"文件不存在：{config['resume']}"
           resume_result = torch.load(config['resume'])
           model.load_state_dict(resume_result['model'])
           optimizer.load_state_dict(resume_result['optimizer'])
           scheduler.load_state_dict(resume_result['scheduler'])
           best_ckpt_path = config['resume']
           best_f1 = resume_result['metric_dict']['f1']

       pbar = tqdm(dynamic_ncols=True, total=total_steps)

       for epoch in range(epochs):
           # model 设置为训练模式
           model.train()
           pbar.set_description(f'[{epoch + 1}/{epochs}]')
           for item in train_loader:
               texts, labels = item['text'], item['label']
               model_inputs = tokenizer(
                   texts,
                   padding=True,
                   truncation=True,
                   max_length=config['plm_config']['max_length'],
                   return_tensors="pt"
               )

               for key in model_inputs.keys():
                   model_inputs[key] = model_inputs[key].to(device)

               labels = torch.LongTensor(labels).to(device) - 1
               outputs = model(**model_inputs, labels=labels)
               loss = outputs.loss

               pbar.set_postfix({'loss': round(loss.item(), 4)})
               pbar.update(1)

               optimizer.zero_grad()
               loss.backward()
               optimizer.step()
               scheduler.step()

           metric_dict = do_evaluate(
               model,
               tokenizer,
               valid_loader,
               config['plm_config']['max_length'],
               device
           )

           pp(metric_dict)

           if metric_dict['f1'] > best_f1:
               best_f1 = metric_dict['f1']

               if os.path.exists(best_ckpt_path):
                   os.remove(best_ckpt_path)

               best_ckpt_path = f"{config['output_dir']}/epoch_{epoch + 1}_f1_{round(best_f1, 4)}.pth"

               torch.save({
                   'model': model.state_dict(),
                   'optimizer': optimizer.state_dict(),
                   'scheduler': scheduler.state_dict(),
                   'metric_dict': metric_dict
               }, best_ckpt_path)

       # 保存最佳模型的路径
       return best_ckpt_path

   ```

5. 模型验证

   ```python
   @torch.no_grad()
   def do_evaluate(model, tokenizer, valid_loader, max_length, device) -> dict:
       """
           评估模型
           :param model: 模型
           :param tokenizer: 分词器
           :param valid_loader: 验证集
           :param max_length: 最大长度
           :param device: 设备
           :return: 评估结果
       """
       # model 设置为评估模式
       model.eval()
       total_predict, total_ground = [], []
       for item in tqdm(valid_loader, dynamic_ncols=True, desc='evaluating...'):
           texts, labels = item['text'], item['label']
           model_inputs = tokenizer(
               texts,
               padding=True,
               truncation=True,
               max_length=max_length,
               return_tensors="pt"
           )
           for key in model_inputs.keys():
               model_inputs[key] = model_inputs[key].to(device)

           labels = torch.LongTensor(labels).to(device) - 1
           outputs = model(**model_inputs, labels=labels)
           logits = outputs.logits
           predict = logits.argmax(dim=-1).cpu().numpy().tolist()
           total_predict.extend(predict)
           total_ground.extend(labels.cpu().numpy().tolist())
       return {
           "f1": f1_score(total_ground, total_predict, average='macro'),
           "acc": accuracy_score(total_ground, total_predict),
           "recall": recall_score(total_ground, total_predict, average='macro'),
           "precision": precision_score(total_ground, total_predict, average='macro')
       }

   ```

### 完整训练代码如下：

```python
# -*-coding: Utf-8 -*-
# @Project Name: d2l-zh
# @File: __init__.py.py
# @Author: David
# @Date：2023/8/31 23:13
import os
from pprint import pp

import hydra
import pandas as pd
import torch
from omegaconf import OmegaConf
from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score
from torch import nn
from torch.optim.adamw import AdamW
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from transformers import XLMRobertaForSequenceClassification, XLMRobertaConfig, XLMRobertaTokenizerFast, get_linear_schedule_with_warmup
from transformers.models.roberta.modeling_roberta import RobertaClassificationHead


def split_dataset(data_path: str = "data/data.csv", ratio=0.8):
    """
        划分数据集
        :param ratio: 划分比例
        :param data_path: 数据集路径
        :return:
    """
    # 读取文件
    data = pd.read_csv(data_path, encoding='GBK', names=['text', 'label'])
    data.dropna(axis=0, inplace=True)  # 删除空行
    data.dropna(axis=1, inplace=True)  # 删除空行

    # 划分数据集
    train_data = data.sample(frac=ratio, random_state=0, axis=0)
    valid_data = data.drop(train_data.index)

    print(f"数据集大小：{data.shape[0]}")
    print(f"训练集大小：{train_data.shape[0]}")
    print(f"验证集大小：{valid_data.shape[0]}")

    # 保存数据集
    train_data.to_csv("data/train.csv", columns=["text", "label"], index=False, encoding='GBK')
    valid_data.to_csv("data/valid.csv", columns=["text", "label"], index=False, encoding='GBK')


def freeze_model(model: nn.Module):
    """
        冻结模型
        :param model: 模型
        :return:
    """
    for p in model.parameters():
        p.requires_grad = False


def load_pretrained_model(config):
    """
        加载预训练语言模型
        :param config: 配置文件
        :return: 模型和分词器
    """
    # 加载配置文件和修改配置文件
    plm_config = XLMRobertaConfig.from_pretrained(config['pretrained_model_name_or_path'])
    for key, value in config['plm_config'].items():
        setattr(plm_config, key, value)

    # 加载预训练语言模型和分词器
    model = XLMRobertaForSequenceClassification.from_pretrained(config['pretrained_model_name_or_path'])
    tokenizer = XLMRobertaTokenizerFast.from_pretrained(config['pretrained_model_name_or_path'])

    # 修改模型的分类器配置文件
    model.plm_config = plm_config
    model.num_labels = plm_config.num_labels
    model.classifier = RobertaClassificationHead(plm_config)

    # 冻结模型
    freeze_model(model.roberta)

    # 返回模型和分词器
    return model, tokenizer


class MyDataset(Dataset):
    def __init__(self, csv_file_path):
        super(MyDataset, self).__init__()
        df = pd.read_csv(csv_file_path, encoding="GBK")
        self.texts = df['text'].values.tolist()
        self.labels = df['label'].values.tolist()
        self.size = df.shape[0]

    def __getitem__(self, idx):
        return {
            'text': self.texts[idx],
            'label': self.labels[idx]
        }

    def __len__(self):
        return self.size


def load_data(config):
    train_path, valid_path = config['train_file_path'], config['valid_file_path']
    train_dataset = MyDataset(train_path)
    valid_dataset = MyDataset(valid_path)
    train_loader = DataLoader(
        dataset=train_dataset,
        shuffle=True,
        drop_last=False,
        batch_size=config['train_batch_size']
    )
    valid_loader = DataLoader(
        dataset=valid_dataset,
        shuffle=False,
        drop_last=False,
        batch_size=config['valid_batch_size']
    )

    return train_loader, valid_loader


def do_train(config):
    epochs, device = config['epochs'], torch.device(config['device'])

    # 加载数据集
    train_loader, valid_loader = load_data(config)

    # 计算总步数
    total_steps = len(train_loader) * epochs

    # 加载模型
    model, tokenizer = load_pretrained_model(config)
    model.to(device)

    # 加载优化器
    optimizer = AdamW(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])
    scheduler = get_linear_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=config['warm_up_steps'],
        num_training_steps=total_steps
    )

    best_ckpt_path, best_f1 = "", 0

    # 断续训练
    if config['resume']:
        assert os.path.exists(config['resume']), f"文件不存在：{config['resume']}"
        resume_result = torch.load(config['resume'])
        model.load_state_dict(resume_result['model'])
        optimizer.load_state_dict(resume_result['optimizer'])
        scheduler.load_state_dict(resume_result['scheduler'])
        best_ckpt_path = config['resume']
        best_f1 = resume_result['metric_dict']['f1']

    pbar = tqdm(dynamic_ncols=True, total=total_steps)

    for epoch in range(epochs):
        # model 设置为训练模式
        model.train()
        pbar.set_description(f'[{epoch + 1}/{epochs}]')
        for item in train_loader:
            texts, labels = item['text'], item['label']
            model_inputs = tokenizer(
                texts,
                padding=True,
                truncation=True,
                max_length=config['plm_config']['max_length'],
                return_tensors="pt"
            )

            for key in model_inputs.keys():
                model_inputs[key] = model_inputs[key].to(device)

            labels = torch.LongTensor(labels).to(device) - 1
            outputs = model(**model_inputs, labels=labels)
            loss = outputs.loss

            pbar.set_postfix({'loss': round(loss.item(), 4)})
            pbar.update(1)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

        metric_dict = do_evaluate(
            model,
            tokenizer,
            valid_loader,
            config['plm_config']['max_length'],
            device
        )

        pp(metric_dict)

        if metric_dict['f1'] > best_f1:
            best_f1 = metric_dict['f1']

            if os.path.exists(best_ckpt_path):
                os.remove(best_ckpt_path)

            best_ckpt_path = f"{config['output_dir']}/epoch_{epoch + 1}_f1_{round(best_f1, 4)}.pth"

            torch.save({
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'scheduler': scheduler.state_dict(),
                'metric_dict': metric_dict
            }, best_ckpt_path)

    # 保存最佳模型的路径
    return best_ckpt_path


@torch.no_grad()
def do_evaluate(model, tokenizer, valid_loader, max_length, device) -> dict:
    """
        评估模型
        :param model: 模型
        :param tokenizer: 分词器
        :param valid_loader: 验证集
        :param max_length: 最大长度
        :param device: 设备
        :return: 评估结果
    """
    # model 设置为评估模式
    model.eval()
    total_predict, total_ground = [], []
    for item in tqdm(valid_loader, dynamic_ncols=True, desc='evaluating...'):
        texts, labels = item['text'], item['label']
        model_inputs = tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        )
        for key in model_inputs.keys():
            model_inputs[key] = model_inputs[key].to(device)

        labels = torch.LongTensor(labels).to(device) - 1
        outputs = model(**model_inputs, labels=labels)
        logits = outputs.logits
        predict = logits.argmax(dim=-1).cpu().numpy().tolist()
        total_predict.extend(predict)
        total_ground.extend(labels.cpu().numpy().tolist())
    return {
        "f1": f1_score(total_ground, total_predict, average='macro'),
        "acc": accuracy_score(total_ground, total_predict),
        "recall": recall_score(total_ground, total_predict, average='macro'),
        "precision": precision_score(total_ground, total_predict, average='macro')
    }


def check():
    """
        检查数据集
    """
    data = pd.read_csv("data/valid.csv", encoding='GBK', names=['text', 'label'])

    # 按行遍历 data
    for i, row in data.iterrows():
        if isinstance(row['text'], float) or isinstance(row['label'], float):
            print(f"text or label is empty: {i}")
            print(f"text {row['text']}, label {row['label']}")


@hydra.main(
    config_path='config',  # 配置文件路径
    config_name='config',  # 配置文件名称
    version_base='1.3.2'  # 版本号
)
def main(config):
    config = OmegaConf.to_container(config, resolve=True)

    # 创建输出目录
    os.makedirs(config['output_dir'], exist_ok=True)

    # split_dataset() # 划分数据集
    do_train(config)  # 训练模型


if __name__ == '__main__':
    split_dataset()
    check()
    main()

```

## 2. 发现新的软件使用技巧

`Typora` 软件可以打开控制台，本质上和浏览器没有区别，只是一个套壳的浏览器而已

![image-20230901014434093](https://cdn.jsdelivr.net/gh/David-deng-01/images/blogimage-20230901014434093.png)

![image-20230901014452319](https://cdn.jsdelivr.net/gh/David-deng-01/images/blogimage-20230901014452319.png)

## 3. 错误解决

错误提示：`TypeError: TextEncodeInput must be Union[TextInputSequence,Tupele[InputSequence, InputSequence]]`

问题原因：数据集中存在空值或者 `nan`

解决方案：处理数据时，删除数据中的空值

```python
def split_dataset(data_path: str = "data/data.csv", ratio=0.8):
    """
        划分数据集
        :param ratio: 划分比例
        :param data_path: 数据集路径
        :return:
    """
    # 读取文件
    data = pd.read_csv(data_path, encoding='GBK', names=['text', 'label'])

    # here
    data.dropna(axis=0, inplace=True)  # 删除空行
    data.dropna(axis=1, inplace=True)  # 删除空行

    # 划分数据集
    train_data = data.sample(frac=ratio, random_state=0, axis=0)
    valid_data = data.drop(train_data.index)

    print(f"数据集大小：{data.shape[0]}")
    print(f"训练集大小：{train_data.shape[0]}")
    print(f"验证集大小：{valid_data.shape[0]}")

    # 保存数据集
    train_data.to_csv("data/train.csv", columns=["text", "label"], index=False, encoding='GBK')
    valid_data.to_csv("data/valid.csv", columns=["text", "label"], index=False, encoding='GBK')

```

