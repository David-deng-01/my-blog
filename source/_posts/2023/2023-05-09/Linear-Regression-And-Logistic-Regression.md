---
title: 线性回归和逻辑回归
layout: post
tags:
  - Educoder
  - ML
categories:
  - Educoder
  - ML
lang: zh-CN
mathjax: true
abbrlink: 33786
date: 2023-05-09 22:50:47
---
# 线性回归和逻辑回归

## 1. 线性回归(Linear Regression)

### 什么是回归（Regression）

**回归分析**是一种统计方法,于对具有一个或多个自变量的因变量(目标变量)和自变量(预测变量)之间的关系进行建模。具体地说，回归分析有助于我们理解在其他自变量保持固定的情况下，自变量的值对应于自变量的变化方式。它可以预测连续/实际值,例如温度,年龄,工资,价格等。

通俗理解：越来越接近期望值的过程，回归于事物本来的面目。

回归是一种有监督的学习技术，有助于发现变量之间的相关性,并使我们能够基于一个或多个预测变量来预测连续输出变量。它主要用于预测，时间序列建模以及确定变量之间的因果关系。

在回归中，我们在最适合给定数据点的变量之间绘制图形,使用此图形，机器学习模型可以对数据进行预测。用简单的话说，回归显示一条线或曲线， 它穿过目标预测图上的所有数据点，以使数据点和回归线之间的垂直距离最小。数据点和线之间的距离表明模型是否已捕获牢固的关系。

主要用于预测数值型数据，典型的回归例子：数据拟合曲线

### 什么是线性回归（Linear Regression）

在统计学中，线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为**简单回归**，大于一个自变量情况的叫做**多元回归**。（这反过来又应当由多个相关的因变量预测的多元线性回归区别，而不是一个单一的标量变量。）

在线性回归中，数据使用线性预测函数来建模，并且未知的模型参数也是通过数据来估计。这些模型被叫做**线性模型**。最常用的线性回归建模是给定X值的y的条件均值是X的仿射函数。不太一般的情况，线性回归模型可以是一个中位数或一些其他的给定X的条件下y的条件分布的分位数作为X的线性函数表示。像所有形式的回归分析一样，线性回归也把焦点放在给定X值的y的条件概率分布，而不是X和y的联合概率分布（多元分析领域）。

线性回归是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。这是因为线性依赖于其未知参数的模型比非线性依赖于其未知参数的模型更容易拟合，而且产生的估计的统计特性也更容易确定。

线性回归模型经常用**最小二乘**逼近来拟合，但他们也可能用别的方法来拟合，比如用最小化“拟合缺陷”在一些其他规范里（比如最小绝对误差回归），或者在桥回归中最小化最小二乘损失函数的惩罚.相反,最小二乘逼近可以用来拟合那些非线性的模型.因此，尽管“最小二乘法”和“线性模型”是紧密相连的，但他们是不能划等号的。

> 最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。
>
> 最小二乘法还可用于曲线拟合，其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达 。

线性回归假设输出变量是若干输出变量的线性组合，并根据这一关系求解线性组合中的最优系数。

通俗理解：输出一个线性函数，例如

<img src="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/image-20230511231459904.png" alt="image-20230511231459904" style="zoom:100%;" />

假定一个实例可以用列向量表示，每个代表实例在第i个属性上的取值，线性回归就是都求出一组参数，使预测输出可以表示为以这组参数为权重的实例属性的线性组合。例如引入常量，线性回归试图学习的模型就是

- 当实例只有一个属性时，输入输出之间关系就是二维平面的一条直线
- 当实例属性数目较多时，得到的是n+1维空间的一个超平面，对应一个维度等于于n的线性子空间

### 什么是一元线性回归（Unary Linear Regression）

一元线性回归也叫单变量线性回归，一元线性回归是分析只有一个自变量。从一个输入值预测一个输出值，输入/输出的对应关系就是一个线性函数。例如：一个经济指标的数值往往受许多因素影响，若其中只有一个因素是主要的，起决定性作用，则可用一元线性回归进行预测分析。

<img src="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/image-20230511231831139.png" alt="image-20230511231831139" style="zoom:100%;" />

### 什么是多元线性回归（Multiple Linear Regression）

在回归分析中，如果有两个或两个以上的自变量，就称为多元回归。当样本的描述涉及多个属性时，这类问题就被称为多元线性回归。。事实上，一种现象常常是与多个因素相联系的，由多个自变量的最优组合共同来预测或估计因变量，比只用一个自变量进行预测或估计更有效，更符合实际。因此多元线性回归比一元线性回归的实用意义更大。例如，肌肉是睡眠、饮食、训练多种因素共同作用的结果。

<img src="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/image-20230511232444558.png" alt="image-20230511232444558" style="zoom:100%;" />

## 2. 逻辑回归(Logistic Regression)

### 什么是逻辑回归（Logistic Regression）

线性回归能对连续值进行预测，而现实中学常见的另一类问题是分类，逻辑回归解决的就是分类问题。逻辑回归输出的实例属于每个类别的似然概率，似然概率最大的类别就是分类结果。通俗理解：逻辑就是True或False，判断出是True还是False，相当于分类了。在二分类任务中，逻辑回归可以视为在平面直角坐标系上划定一条数据分类的判定边界。

逻辑回归就是这样的一个过程：面对一个回归或者分类问题，建立代价函数，然后通过优化方法迭代求解出最优的模型参数，然后测试验证我们这个求解的模型的好坏。Logistic回归虽然名字里带“回归”，但是它实际上是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别）回归模型中，y是一个定性变量，比如y=0或1，逻辑回归方法主要应用于研究某些事件发生的概率。

<img src="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/image-20230511233440196.png" alt="image-20230511233440196" style="zoom:100%;" />



## 3. 线性回归和逻辑回归的区别

线性回归和逻辑回归的性质不同、任务定位不同、输出值不同、损失函数不同等。下面简单描述两种机器学习方法的不同点。

### 性质不同

1. 线性回归模型一般用于预测，用于对连续值进行预测，例如预测房价走势。逻辑回归一般用于分类，用于对离散值进行预测，例如：是否是乳腺癌预测
2. 逻辑回归是一种广义的线性回归分析模型；线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。
3. 逻辑回归常用于数据挖掘，疾病自动诊断，经济预测等领域；线性回归常运用于数学、金融、趋势线、经济学等领域。

### 任务定位不同

1. 线性回归用于回归任务；逻辑回归用于分类任务。

### 输出值不同

1. 线性回归输出连续值；
2. 逻辑回归输出概率值；

### 损失函数不同

1. 线性回归一般采用Mean Square Error Loss(均方误差损失)函数
2. 逻辑回归采用Cross Entropy Loss(交叉熵损失)函数

## 4. 线性回归和逻辑回归的联系

本质上，逻辑回归是将线性回归的结果通过 `sigmod` 函数进行了映射，将值域映射到(0,1)，在二类任务中，若大于0.5，则为某个类，小于0.5，为另一类。

## 4. 总结

线性回归和逻辑回归都是总要的机器学习方法，两种方法既有区别又有联系。它们适用于不同的任务，有不同的效果。但是本质上两种回归方法是一致的，只是逻辑回归方法在线性回归方法的基础上，对线性回归方法的结果进行了映射。

