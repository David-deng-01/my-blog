---
title: 如何评价数据标注中的一致性? 以信息抽取为例，浅谈 Fleiss' Kappa
layout: post
tags:
  - 转载
  - 知乎
categories:
  - 转载
  - 知乎
lang: zh-CN
abbrlink: 21342
date: 2023-10-11 16:30:00
---

# 如何评价数据标注中的一致性？以信息抽取为例，浅谈 Fleiss' Kappa

## 前言

机器学习中涉及新数据集发布的论文通常会描述数据集的构建过程，一份数据集往往由多位标注员共同完成，不同标注员对数据的理解很容易存在偏差，这极大程度上会影响数据集的一致性，从而限制算法的性能。因此在构建数据集的标注过程中，大多数构建数据集的工作都会对标注员之间的标注一致性进行评估，以保证数据集的质量。

假如有三位标注员进行实体识别任务的标注，让他们标注相同的文本，如果他们的标注结果（以 BIO 框架为例）大多数情况如下图所示：

![](https://pic2.zhimg.com/v2-3e8a02498e6ed9320fc9b739ab531b11_b.jpg)

那么很容易猜想：这三位标注员的背景知识、对实体类型的理解和标注原则各有差异，由他们各自标注的数据所组成的数据集会存在非常糟糕的一致性，从而给模型的训练和评估带来极大的噪声。理想中，多位标注员对相同的文本应该产生如下图所示的一致结果：

![](https://pic2.zhimg.com/v2-d071cbd0ab80cffaeb4c7da5ab1c56c1_b.jpg)

然而，极致的一致性可遇而不可求，我们只能希望标注员之间的一致性尽可能地高，那么如何对一致性进行评估呢？这正是本文所要介绍的。

在统计学中，**标注一致性（Inter-Rater Agreement）**用来评价独立观察者之间对同一现象进行评估时的一致程度。因此本文讨论的是多位标注员对相同数据进行标注时的一致性评估方法。如果多位标注员在相同的数据中达到符合要求的一致程度，那么我们可以认为这些标注员能够独立负责各自的数据标注工作，由他们的标注结果所构成的数据集是符合一致性要求的。本文主要介绍**Kappa**统计量，由浅入深依次介绍**Cohen's Kappa**，**Scott's Pi**和**Fleiss' Kappa**。最后以文本信息抽取为例，浅谈**Fleiss' Kappa**在实体识别任务中的应用。

## 1\. Kappa 概述

先来思考一个问题：评估一致性最简单且直观的方法是什么？统计标注一致的样本占所有标注样本的百分比。既然如此简单，那我们为什么还要引入**Kappa**呢？这是因为**Kappa**考虑了由随机因素导致的一致性。假设我们通过投掷硬币的方法来预测股票涨跌，那么硬币的预测结果与真实情况的一致性显然不可能为零，这就是随机因素导致的。

**Kappa**是用来描述定性（分类）任务中标注一致性的统计量，记作 $\kappa$ 。

$$
\kappa=1-\frac{1-p_{o}}{1-p_{e}}=\frac{p_{o}-p_{e}}{1-p_{e}}\\
$$

其中 $p_{o}$ 代表观察到的标注员之间的一致性， $p_{e}$ 代表基于假设的、由随机因素导致的一致性。通过观察上式可以发现当 $p_{o}=1$ 时，$\kappa=1$ 表明标注结果完全一致。当标注结果由随机性主导时 $p_{o}=p_{e}$ ， $\kappa=0$ 表明标注结果完全没有一致性（观察到的一致性是由随机性导致的）。当然 $\kappa<0$ 的情况也有可能出现，这意味着标注员之间倾向于给出完全不一致的标注结果。

关于**Kappa**的变体都是围绕着 $p_{o}$ 和 $p_{e}$ 的不同计算方法展开的。**Cohen's Kappa**只能用来评估两位标注员之间的一致性，**Scott's Pi**相对于**Cohen's Kappa**采用不同策略计算随机性 $p_{e}$ ，**Fleiss' Kappa**则在**Scott's Pi**的基础上进行泛化，使其能对任意固定数量的标注员评估一致性。

## 2\. Cohen's Kappa

该统计量评估两位标注员对 $N$ 条数据做多分类（定性）任务时的一致性。 $p_{o}$ 是标注一致的样本占所有标注样本的百分比。随机一致性 $p_{e}$ 如下式所示计算，其中 $k$ 为类别的数量。

$$
p_{e}=\sum_{k}^{}{\tilde{p_{k12}}}=\sum_{k}^{}{\tilde{p_{k1}}\tilde{p_{k2}}}=\sum_{k}^{}{\frac{n_{k1}}{N}\frac{n_{k2}}{N}}\\
$$

$\tilde{p_{k12}}$ 代表估计标注员 $1$ 和 $2$ 把同一条数据归为第 $k$ 个类的概率。基于独立假设，即：两位标注员的标注是独立且互不影响的，那么有 $\tilde{p_{k12}}=\tilde{p_{k1}}\tilde{p_{k2}}$ 。 $\tilde{p_{k1}}=\frac{n_{k1}}{N}$ 通过 $N$ 条数据中标注员 $1$ 标注 $k$ 的数量 $n_{k1}$ 来评估。

### 2.1 举例

假设有 $A$ 和 $B$ 两位标注员对 50 条数据进行分类，分为 $Yes$ 和 $No$ 两类，下图为标注结果矩阵：

![](https://pic1.zhimg.com/v2-2c9e8bb73d940920fbab2626d943325c_b.jpg)

标注一致性 $p_{o}$ ：

$$
p_{o}=\frac{a+d}{a+b+c+d}=\frac{20+15}{50}=0.7\\
$$

随机一致性 $p_{e}$ ：

$$
p_{Yes}=\frac{a+b}{a+b+c+d}\cdot\frac{a+c}{a+b+c+d}=0.5\times0.6=0.3\\
$$

$$
p_{No}=\frac{c+d}{a+b+c+d}\cdot\frac{b+d}{a+b+c+d}=0.5\times0.4=0.2\\
$$

$$
p_{e}=p_{Yes}+p_{No}=0.3+0.2=0.5\\
$$

**Kappa**（ $\kappa$ ）：

$$
\kappa=\frac{p_{o}-p_{e}}{1-p_{e}}=\frac{0.7-0.5}{1-0.5}=0.4\\
$$

### 2.2 问题

**Cohen's Kappa**存在一个明显问题：如果两位标注员的一致性 $p_{o}$ 相同，那么他们标注的标签分布越趋于一致， $\kappa$ 理应越高。然而事实并非如此，这会导致更高的随机一致性 $p_{e}$ ，从而使 $\kappa$ 下降。

如下图所示，当标注员之间的一致性 $p_{o}$ 相同，且标注员 $A$ 的标签分布为（60%-Yes，40%-No）保持不变时，标注员 $B$ 的标签分布由（70%-Yes，30%-No）变为（30%-Yes，70%-No），这种与 $A$ 趋于相反的标签分布变化反而提升了 $\kappa$ 的值，这是显然违背直觉的。

![](https://pic1.zhimg.com/v2-87a6995cec5adfe0d6a95877d9652738_b.jpg)

## 3\. Scott's Pi

为了优化**Cohen's Kappa**的上述问题，采用不同策略计算 $p_{e}$ 。如下式所示，通过标签的联合边缘分布来估计随机一致性，也可以说**Scott's Pi**是算数平均值的平方，而**Cohen's Kappa**是几何平均值的平方。

$$
p_{e}=\sum_{k}^{}{(\frac{n_{k1}+n_{k2}}{N})^{2}}\\
$$

### 3.1 举例

假设有 $A$ 和 $B$ 两位标注员对 45 条数据进行分类，分为 $Yes$ 、 $No$ 和 $Maybe$ 三类，下图为标注结果矩阵：

![](https://pic4.zhimg.com/v2-4d3eeefe6bbd22fe3923ee2dd9a7bcdb_b.jpg)

Marginal Sum 为标签的边缘分布

![](https://pic1.zhimg.com/v2-6699e04d6ad505e0279ed3dd9027e85c_b.jpg)

![](https://pic1.zhimg.com/80/v2-6699e04d6ad505e0279ed3dd9027e85c_720w.webp)

Joint Proportion(JP) 为联合边缘分布，Squared 对其求平方

标注一致性 $p_{o}$ ：

$$
p_{o}=\frac{1+5+9}{45}=0.333\\
$$

**Kappa**（ $\kappa$ ）：

$$
\kappa=\frac{p_{o}-p_{e}}{1-p_{e}}=\frac{0.333-0.369}{1-0.369}=-0.057\\
$$

### 3.2 问题

**Cohen's Kappa**和**Scott's Pi**只能评估两位标注员之间的一致性，无法拓展到多位标注员。

## 4\. Fleiss' Kappa

对**Scott's Pi**进行泛化，可以评估多位标注员之间的标注一致性。假设有 $N$ 条数据进行多分类任务，每条数据被标注了 $n$ 次， $k$ 为类别数量。标注数据的索引为 $i=1,2,...,N$ ，类别的索引为 $j=1,2,...k$ ，那么令 $n_{ij}$ 表示将第 $i$ 条数据被标注为类别 $j$ 的次数。

随机一致性 $p_{e}$ 的评估方法与**Scott's Pi**相同， $p_{j}$ 是类别 $j$ 的联合边缘分布：

$$
p_{j}=\frac{1}{Nn}\sum_{i=1}^{N}{n_{ij}}\\
$$

$$
p_{e}=\sum_{j=1}^{k}{p_{j}^{2}}\\
$$

标注一致性 $p_{o}$ 是每条数据的一致性 $p_{i}$ 的均值：

$$
p_{i}=\frac{1}{n(n-1)}\sum_{j=1}^{k}{n_{ij}(n_{ij}-1)}\\
$$

已知第 $i$ 条数据被标注 $n$ 次，任取一位标注员，则有 $n-1$ 位其他标注员与该位标注员的标注结果一致或不一致，因此 $n(n-1)$ 代表所有标注对（Annotation Pairs）的数量。同理，对于类别 $j$ 来说，有 $n_{ij}(n_{ij}-1)$ 个一致的标注对。 $p_{i}$ 是各类别一致的标注对占所有标注对的百分比，化简后有：

$$
p_{i}=\frac{1}{n(n-1)}[(\sum_{j=1}^{k}{n_{ij}^{2})-n}]\\
$$

$$
p_{o}=\frac{1}{N}\sum_{i=1}^{N}{p_{i}}\\
$$

### 4.1 举例

假设有 14 $(n)$ 位标注员对 10 $(N)$ 条数据进行类别数量为 5 $(k)$ 的多分类标注，如下图所示：行（Rows）为数据索引 $i$ ，列（Columns）为类别索引 $j$ ，单元格的值为 $n_{ij}$（标注次数）。

![](https://pic2.zhimg.com/v2-a0d0d8cc84a8d286ad79c4308f4174e1_b.jpg)

![](https://pic2.zhimg.com/80/v2-a0d0d8cc84a8d286ad79c4308f4174e1_720w.webp)

随机一致性 $p_{e}$，在计算 $p_{j}$ 时以 $p_{1}$ 为例：

$$
p_{j=1}=\frac{0+0+0+0+2+7+3+2+6+0}{140}=0.143\\
$$

$$
p_{e}=0.143^{2}+0.200^{2}+0.279^{2}+0.150^{2}+0.229^{2}=0.213\\
$$

标注一致性 $p_{o}$ ，在计算 $p_{i}$ 时以 $p_{2}$ 为例：

$$
p_{i=2}=\frac{1}{14(14-1)}(0^{2}+2^{2}+6^{2}+4^{2}+2^{2}-14)=0.253\\
$$

$$
p_{o}=\frac{1}{N}\sum_{i=1}^{N}{p_{i}=}\frac{1}{10}(1.000+0.253+...+0.286)=0.378\\
$$

**Kappa**（ $\kappa$ ）：

$$
\kappa=\frac{p_{o}-p_{e}}{1-p_{e}}=\frac{0.378-0.213}{1-0.213}=0.210\\
$$

### 4.2 分析

最后一个问题： $\kappa$ 的取值应该如何理解？下图是从主观和经验上对 $\kappa$ 取值的解释，通常认为当 $\kappa>0.8$ 时，标注员之间有几乎完美的一致性。$\kappa$ 越趋近于 1，意味着标注一致性越高。

![](https://pic3.zhimg.com/v2-b4cf93b362710f29c8650ac3fefd3422_b.jpg)

## 5\. 在信息抽取中的应用

以实体识别为例，虽然实体识别是一个序列标注问题，但我们可以将该任务视为对每个 token 的多分类任务。使用**Fleiss' Kappa**评估一致性时，最关键的步骤是构建如 4.1 节所示的：对象-类别矩阵（Objects-Categories Matrix）。在实体识别中，标注对象可以理解为 token，回到本文最初例举的标注案例：

![](https://pic2.zhimg.com/v2-3e8a02498e6ed9320fc9b739ab531b11_b.jpg)

其标注结果矩阵为：

![](https://pic1.zhimg.com/v2-2c5d4f26825b10b8f3face99ba28ee2c_b.jpg)

在真实场景中，实体识别任务通常有大量的非实体（被标注为'O'的 token）。这会导致标签的不平衡情况，因此我们可以进一步忽略那些被所有标注员标注为'O'的 token。

如何对嵌套的实体标注进行一致性评估呢？我们可以以连续片段（Span）为标注对象。假设句子的长度为 $n$ ，那么该序列有 $n(n+1)/2$ 个待标注的候选片段，去除大量的非实体片段（负样本）后，其构建的标注结果矩阵如下：

![](https://pic2.zhimg.com/v2-dee8c73b5c7fd0a09cce47ad19a1dfdd_b.jpg)

---

原文出处：[如何评价数据标注中的一致性？以信息抽取为例，浅谈 Fleiss' Kappa - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/547781481)
