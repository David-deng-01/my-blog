<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>多模态基础模型：从专家到通用助理 | David 的博客</title><meta name="author" content="David"><meta name="copyright" content="David"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="多模态大模型综述">
<meta property="og:type" content="article">
<meta property="og:title" content="多模态基础模型：从专家到通用助理">
<meta property="og:url" content="https://david-deng.cn/2024/01/04/2024-01-05%20%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/index.html">
<meta property="og:site_name" content="David 的博客">
<meta property="og:description" content="多模态大模型综述">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-4.png">
<meta property="article:published_time" content="2024-01-04T14:42:08.000Z">
<meta property="article:modified_time" content="2024-01-04T14:45:16.000Z">
<meta property="article:author" content="David">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-4.png"><link rel="shortcut icon" href="/img/favicon.svg"><link rel="canonical" href="https://david-deng.cn/2024/01/04/2024-01-05%20%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":90,"position":"top","messagePrev":"文章距离最近一次更新已经","messageNext":"天，文章的内容可能已经过期。"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: David","link":"链接: ","source":"来源: David 的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '多模态基础模型：从专家到通用助理',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-04 22:45:16'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><style type="text/css">#toggle-sidebar {bottom: 80px}</style><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/loading.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">28</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">27</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-compass"></i><span> 目录</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="https://david-deng.cn/wallpaper/mk-wallpaper/"><i class="fa-fw fas fa-images"></i><span> 孟坤壁纸</span></a></li><li><a class="site-page child" href="https://david-deng.cn/wallpaper/xben-wallpaper/"><i class="fa-fw fas fa-images"></i><span> 小笨壁纸</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-4.png')"><nav id="nav"><span id="blog-info"><a href="/" title="David 的博客"><span class="site-name">David 的博客</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-compass"></i><span> 目录</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fa fa-heartbeat"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="https://david-deng.cn/wallpaper/mk-wallpaper/"><i class="fa-fw fas fa-images"></i><span> 孟坤壁纸</span></a></li><li><a class="site-page child" href="https://david-deng.cn/wallpaper/xben-wallpaper/"><i class="fa-fw fas fa-images"></i><span> 小笨壁纸</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">多模态基础模型：从专家到通用助理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-01-04T14:42:08.000Z" title="发表于 2024-01-04 22:42:08">2024-01-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-04T14:45:16.000Z" title="更新于 2024-01-04 22:45:16">2024-01-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>34分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="多模态基础模型：从专家到通用助理"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Multimodal-Foundation-Models-From-Specialists-to-General-Purpose-Assistants-1-多模态基础模型：从专家到通用助理"><a href="#Multimodal-Foundation-Models-From-Specialists-to-General-Purpose-Assistants-1-多模态基础模型：从专家到通用助理" class="headerlink" title="Multimodal Foundation Models: From Specialists to General-Purpose Assistants[^1]多模态基础模型：从专家到通用助理"></a>Multimodal Foundation Models: From Specialists to General-Purpose Assistants[^1]<br/><span style="font-size: 24px;">多模态基础模型：从专家到通用助理</span ></h1><img src="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/%E6%88%AA%E5%9B%BE-1702886821052.png" alt="截图-1702886821052" style="zoom:100%;" />

<h2 id="0-文章简介Paper-information"><a href="#0-文章简介Paper-information" class="headerlink" title="0. 文章简介Paper information"></a>0. 文章简介<br/><span style="font-size:22px;">Paper information</span></h2><blockquote>
<ol>
<li>文章<code>2023年9月18日</code>发表在 <a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/">arxiv</a>上，目前还是预印版的状态，可以免费的从<a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/2309.10020">链接</a>下载到文章的原文</li>
<li>文章的作者是<code>7名</code>华人，都是微软公司的研究员</li>
<li>文章总长度是<code>119页</code>，其中正文<code>92 页</code>，参考文献<code>27页</code>，</li>
<li>文章一共<code>7个</code>章节，引用了<code>432篇</code>参考文献</li>
<li>关于这篇综述的适用读者，作者的原话是：<code>只要你想学习多模‍态基础模型的基础知识和最新进展，不管你是专业研究员，还是在校学生，它都是你的“菜”。</code>所以这篇文章非常适合我们现在这个阶段了解和学习多模态基础模型的基础只是和最新进展。</li>
</ol>
</blockquote>
<p>和上次我的主讲一样，我并不准备直接按照作者论文中的顺序直接念一遍，其中可能会有一些删减和我的一些看法。所以这次主讲我会分为以下三个部分进行：<code>引言</code>，<code>为特定目的预训练的多模态基础模型</code>，<code>发挥通用助理作用的多模式基础模型</code>。</p>
<h2 id="1-引言Introduction"><a href="#1-引言Introduction" class="headerlink" title="1. 引言Introduction"></a>1. 引言<br/><span style="font-size:22px;">Introduction</span></h2><blockquote>
<p>==<strong>注</strong>==：本文中谈到的多模态大模型主要是关于视觉方面的，我的认为是，语音可以通过一些方法或者工具转换成文字，例如：自动生成字幕等。虽然在语音转换到文字的过程中会损失一些信息，例如：说话人的语气变化，语音语调的变化，但是还是能够将说话人说出的每一个字，每一个词都完整的识别下来。但是视觉方面就很难通过方法或者工具转化成文字。</p>
<p>所以，从直觉上来看，视觉模态相对于其他模态来说，识别，分析，理解上更具有挑战性。</p>
<p>视觉是人类和许多生物感知和与世界互动的主要渠道之一。人工智能（AI）的核心愿望之一是开发AI代理来模仿这种有效感知和生成视觉信号的能力，从而推理并与视觉世界交互。例子包括识别场景中的物体和动作，以及创建用于交流的草图和图片。构建具有可视化功能的基础模型是努力实现这一目标的流行研究领域。</p>
</blockquote>
<blockquote>
<p>本文一共包括7个章节，下面简单概括了每个章节的内容：</p>
<ol>
<li><p><strong>引言（Introduction）</strong>:</p>
<ul>
<li>介绍了多模态基础模型的研究背景，强调了这些模型在视觉理解和生成任务中的重要性。</li>
<li>提出了从专家模型向通用助手模型转变的趋势，以及这一转变背后的研究动机和挑战。</li>
<li>概述了多模态基础模型的分类，包括视觉理解、视觉生成、统一视觉模型和多模态代理等。</li>
</ul>
</li>
<li><p><strong>视觉理解（Visual Understanding）</strong>:</p>
<ul>
<li>讨论了通过有标签监督学习、无标签图像-文本对比预训练和图像级自监督学习等方法来学习强大的图像表示。</li>
<li>介绍了不同的学习方法，包括使用图像分类、图像-文本对比预训练（如CLIP）和图像掩模自监督学习（如MAE）等。</li>
<li>探讨了如何通过这些方法提升模型在视觉理解任务上的性能，例如图像分类、对象检测和分割等。</li>
</ul>
</li>
<li><p><strong>视觉生成（Visual Generation）</strong>:</p>
<ul>
<li>描述了文本到图像生成模型的发展，以及如何通过人类指导来提高生成图像的质量。</li>
<li>讨论了生成模型的技术，包括基于GAN（生成对抗网络）、VAE（变分自编码器）和扩散模型等方法。</li>
<li>强调了生成模型在遵循文本描述生成图像时的挑战，以及如何通过改进模型架构和训练策略来解决这些挑战。</li>
</ul>
</li>
<li><p><strong>统一视觉模型（Unified Vision Models）</strong>:</p>
<ul>
<li>探讨了如何设计统一的模型架构来整合视觉理解和生成任务。</li>
<li>介绍了如何通过大型语言模型（LLM）来实现视觉任务的统一，以及这种统一模型在不同视觉任务上的表现。</li>
<li>分析了统一模型的优势和挑战，以及如何通过LLM来提升模型的性能。</li>
</ul>
</li>
<li><p><strong>大型多模态模型：与LLM一起训练（Large Multimodal Models: Training with LLM）</strong>:</p>
<ul>
<li>介绍了如何通过LLM来训练多模态模型，以及这些模型在理解和生成任务上的表现。</li>
<li>讨论了端到端训练方法，以及如何将LLM与视觉模型结合起来。</li>
<li>分析了这种方法在提高模型在多模态任务上的性能方面的潜力。</li>
</ul>
</li>
<li><p><strong>多模态代理：与LLM链式工具（Multimodal Agents: Chaining Tools with LLM）</strong>:</p>
<ul>
<li>讨论了如何将多模态工具与LLM结合起来，以实现更强大的视觉理解和生成能力。</li>
<li>介绍了如何通过链式工具来增强模型的功能，以及这种方法在实际应用中的潜力。</li>
<li>分析了多模态代理在工具使用、性能提升和应用多样性方面的优势。</li>
</ul>
</li>
<li><p><strong>结论和研究趋势（Conclusions and Research Trends）</strong>:</p>
<ul>
<li>总结了多模态基础模型的现状，包括它们在视觉理解和生成任务上的最新进展。</li>
<li>展望了未来研究方向，包括如何构建通用目的的AI代理，以及多模态基础模型在这一过程中的作用。</li>
<li>提出了对多模态基础模型未来发展的见解，包括如何通过技术创新和研究合作来推动这一领域的发展。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<p>随着ChatGPT和GPT-4在自然语言处理(NLP)领域取得的重要突破，计算机视觉领域研究人员提出了一个问题，ChatGPT 和GPT-4在视觉、视觉语言和多模态模型方面的对应是什么？</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/%E6%88%AA%E5%9B%BE-1703499425353.png" alt="截图-1703499425353" style="zoom:100%;" />

<h3 id="1-1-什么是多模态基础模型？-What-are-Multimodal-Foundation-Models"><a href="#1-1-什么是多模态基础模型？-What-are-Multimodal-Foundation-Models" class="headerlink" title="1.1 什么是多模态基础模型？ What are Multimodal Foundation Models?"></a>1.1 什么是多模态基础模型？ <br/><span style="font-size:18px;">What are Multimodal Foundation Models?</span></h3><blockquote>
<p><strong>基础模型</strong>：通过大量的数据训练的模型，例如：<code>Bert</code>、<code>GPT系列</code>、<code>CLIP</code>和<code>Dall-E</code>等，这些模型能够适应广泛的下游任务，所以称之为基础模型。[^3]</p>
</blockquote>
<blockquote>
<p>下图中展示了多模态基础模型旨在解决三个典型的问题：<code>视觉理解任务</code>、<code>视觉生成任务</code>和<code>具有语言理解和生成的通用接口</code></p>
<p>**视觉理解(Visual Understanding)**：如何学习视觉表达？</p>
<p>**视觉生成(Visual Generator)**：如何进行视觉生成？</p>
<p>**通用接口(General-purpose Interface)**：</p>
<ol>
<li>如何在没有LLM的情况下，将视觉模型与接口统一起来？ </li>
<li>如何训练多模态大语言模型？</li>
<li>如何将多模态专家和大模型联系起来？</li>
</ol>
<p>从图中我们可以看出具体的流程是：视觉编码器学习输入的视觉表达，输入到语言理解和生成的大语言模型中得到的输出结果输入到视觉生成模块中进行视觉的生成</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/multimodel%20foundation%20models.png" alt="多模态基础模型" style="zoom:100%;" />

<blockquote>
<ol>
<li><strong>视觉理解模型</strong>：学习一般视觉表示对于构建视觉基础模型至关重要，因为预先训练强大的视觉骨干模型(Visual Backbone Model)对于所有计算机视觉下游任务从图像级（例如，图像分类、检索和字幕），区域级（例如，检测和接地）到像素级任务（例如，分段）都是至关重要的。文章将视觉理解模型分成了三类：<ul>
<li>标签监督（Label supervision）</li>
<li>语言监督（Language supervision）</li>
<li>仅使用图片的自监督（Image-only Self-supervision）</li>
</ul>
</li>
<li><strong>视觉生成模型</strong>：<ul>
<li>文本视觉生成（Text-conditioned visual generation），通过文本生成图片和视频</li>
<li>人类对齐视觉生成（Human-aligned visual generation），使模型生成更加符合人类意图，人类价值观的图片和视频</li>
</ul>
</li>
<li><strong>通用接口</strong>：文章提到的通用接口研究集中在以下三个方面：<ul>
<li>用于理解和生成的统一视觉模型（Unified vision models for understanding and generation）通过组合特定用途的多模态模型的功能来构建通用基础模型。</li>
<li>使用大型语言模型进行训练（Training with LLMs）通过将LLM的能力扩展到多模态设置并端到端训练模型，开发了多模态LLM或大型多模态模型，包括<code>Flamingo</code>和多模式<code>GPT-4</code>。</li>
<li>使用大语言模型连接工具（Chaining tools with LLM）利用LLM的工具使用能力，将LLM（如<code>ChatGPT</code>）与各种多模态基础模型集成在一起，以通过对话界面促进图像理解和生成。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<p>基于NLP中的模型发展历史和分类，本文将多模态基础模型分为两类：</p>
<ol>
<li>**专用预训练视觉模型(Specific-Purpose Pre-trained Vision Models)**：涵盖了大多数现有的多模态基础模型，包括视觉理解模型（例如，<code>CLIP</code>、<code>Simplified</code>、<code>BEiT</code>、<code>SAM</code>）和视觉生成模型（例如，Stable Diffusion），因为它们对于特定的视觉问题呈现出强大的可转移能力。</li>
<li>**通用助手(General-Purpose Assistants)**：是指可以遵循人类意图完成各种计算机视觉任务的人工智能代理。通用助理的含义有两个方面：<ul>
<li>具有统一体系结构的模型，可以完成不同问题类型的任务，</li>
<li>遵循人类指令，而不是取代人类。</li>
</ul>
</li>
</ol>
</blockquote>
<h2 id="2-成熟的多模态基础模型研究Well-established-research-of-multimodal-foundation-models"><a href="#2-成熟的多模态基础模型研究Well-established-research-of-multimodal-foundation-models" class="headerlink" title="2. 成熟的多模态基础模型研究Well-established research of multimodal foundation models"></a>2. 成熟的多模态基础模型研究<br/><span style="font-size:22px;">Well-established research of multimodal foundation models</span></h2><h3 id="2-1-视觉理解-Visual-understanding"><a href="#2-1-视觉理解-Visual-understanding" class="headerlink" title="2.1 视觉理解 Visual understanding"></a>2.1 视觉理解 <br/><span style="font-size:18px;">Visual understanding</span></h3><blockquote>
<p>视觉理解方面，作者主要介绍了如何预训练一个强大的图片骨干网络(Image Backbone)，主要是三个方面：</p>
<ol>
<li>标签监督（Label supervision）</li>
<li>语言监督（Language supervision）</li>
<li>仅使用图片的自监督（Image-only Self-supervision）</li>
</ol>
</blockquote>
<blockquote>
<p>下面是原文第二章的组织结构，感兴趣的同学可以查看原文：</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/%E6%88%AA%E5%9B%BE-1703692552675.png" alt="截图-1703692552675" style="zoom:100%;" />

<h4 id="2-1-1-标签监督Label-supervision"><a href="#2-1-1-标签监督Label-supervision" class="headerlink" title="2.1.1 标签监督Label supervision"></a>2.1.1 标签监督<br/><span style="font-size:16px;">Label supervision</span></h4><blockquote>
<p>标签监督的训练方式是最<code>“朴素”</code> 的方式，我们平常训练模型也是使用带有标签的数据集来训练模型，然后通过模型在验证集和测试集上的预测结果对比对应的标签来评价模型的性能。</p>
<p>下面是常见的几个大规模的，有标签的图片数据集：</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/%E6%88%AA%E5%9B%BE-1703748254333.png" alt="截图-1703748254333" style="zoom:100%;" />



<h4 id="2-1-2-语言监督Language-Supervision"><a href="#2-1-2-语言监督Language-Supervision" class="headerlink" title="2.1.2 语言监督Language Supervision"></a>2.1.2 语言监督<br/><span style="font-size:16px;">Language Supervision</span></h4><blockquote>
<p>传统的监督方式可能是给定一个标签，例如：1表示讽刺，0表示不讽刺，的形式进行模型的训练。语言监督(Language Supervision)使用文本作为标签而不是0，1这种编号。因为语言是一种比经典闭集标签更丰富的监督形式，可以直接用于学习可转换的图像表示</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/CLIP.png" alt="CLIP" style="zoom:100%;" />

<blockquote>
<p>从上面的图片中可以看出，左边是CLIP模型的预训练过程，右边是CLIP模型的零样本预测过程。在CLIP模型中，预训练包括两个阶段：视觉预训练和视觉-语言预训练。</p>
<ol>
<li><strong>视觉预训练</strong>：CLIP模型[^4]使用了一个基于Transformer的编码器将图像转换为特征表示，然后通过对比学习的方法，使得同一张图像的不同裁剪或变换之间的距离更近，而不同图像之间的距离更远。这样，模型就能够学习到具有区分度的视觉特征表示。</li>
<li><strong>视觉-语言预训练</strong>：CLIP模型[^4]使用了一个基于Transformer的编码器将图像和文本转换为特征表示，并通过对比学习的方法，使得相同含义的不同图像和文本之间的距离更近，而不同含义的图像和文本之间的距离更远。这样，模型就能够学习到具有良好泛化能力的视觉和语言特征表示，并用于各种视觉和语言任务中。</li>
</ol>
</blockquote>
<blockquote>
<p>OpenAI 使用从互联网上收集到的 4 亿对图像文本对，分别将文本和图像进行编码，之后使用 metric learning 进行训练，其目标是将图像与文本的相似性提高，核心流程比较简洁，可以直接参考下述伪代码：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0. 变量说明</span></span><br><span class="line"><span class="comment"># image_encoder - ResNet或Vision Transformer</span></span><br><span class="line"><span class="comment"># text_encoder - CBOW或 Text Transformer</span></span><br><span class="line"><span class="comment"># I[n, h, w, c] - 小批量对齐图像</span></span><br><span class="line"><span class="comment"># T [n, l] - 小批量对齐文本</span></span><br><span class="line"><span class="comment"># W_i[d_i, d_e] - 学习图像投影以嵌入</span></span><br><span class="line"><span class="comment"># W_t[d_t, d_e] - 学习要嵌入的文本投影</span></span><br><span class="line"><span class="comment"># t - 学习温度参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 提取每个模态的特征表示</span></span><br><span class="line">I_f = image_encoder(I) <span class="comment">#  [n, d_i]</span></span><br><span class="line">T_f = text_encoder(T) <span class="comment"># [n, d_t]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 联合多模式嵌入 [n, d_e]</span></span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=<span class="number">1</span>)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 尺度成对余弦相似度 [n, n]</span></span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 对称损失函数</span></span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=<span class="number">0</span>)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=<span class="number">1</span>)</span><br><span class="line">loss = (loss_i + loss_t)/<span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="2-1-3-仅使用图片的自监督Image-only-Self-supervision"><a href="#2-1-3-仅使用图片的自监督Image-only-Self-supervision" class="headerlink" title="2.1.3 仅使用图片的自监督Image-only Self-supervision"></a>2.1.3 仅使用图片的自监督<br/><span style="font-size:16px;">Image-only Self-supervision</span></h4><blockquote>
<ol>
<li><strong>对比学习</strong>：<ul>
<li><strong>核心思想</strong>：是促进积极的样本对和排斥消极的样本对。除了在CLIP中使用之外，对比学习也是自监督图像表示学习中的流行概念。即：对比学习着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。</li>
<li><strong>优点</strong>：与生成式学习比较，对比学习不需要关注实例上繁琐的细节，只需要在抽象语义级别的特征空间上学会对数据的区分即可，因此模型以及其优化变得更加简单，且泛化能力更强。</li>
<li><strong>目标</strong>：是学习一个编码器，此编码器对同类数据进行相似的编码，并使不同类的数据的编码结果尽可能的不同。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li><strong>非对比学习</strong>：<ol>
<li>非对比学习是一种学习范式，只使用正样本对来训练模型，不像对比度学习那样同时使用正负样本对。这种方法似乎有些反直觉，因为只尝试最小化正样本对之间的距离可能会导致恒定的解。</li>
<li>然而，非对比学习已经被证明可以使用额外的预测器和停止梯度操作来学习非平凡的表示，而且学到的表示在下游任务中表现相当（甚至更好）。</li>
</ol>
</li>
</ol>
<p>==<strong>注</strong>==：非对比学习在原文中的描写也非常的少，如果对非对比学习相关的知识感兴趣的同学可以参考下面两篇文章：</p>
<ol>
<li>SimSiam[^5]：2021年发表在 CVPR</li>
<li>DINO[^6]：2021年发表在ICCV</li>
</ol>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/%E6%88%AA%E5%9B%BE-1704091119451.png" alt="截图-1704091119451" style="zoom:100%;" />

<blockquote>
<ol start="3">
<li><strong>掩码图像建模</strong>：主要包含两个部分，分别是：图像分词器(Image Tokenizer)，掩码预测(Mask Predict)，下面是掩码图像建模模型的执行过程：<ol>
<li>原始图片通过 Tokenizer 转化成 Visual Tokens，作为模型的标签</li>
<li>原始图片进行切片，按照预先确定的规则切分成多个小图片(下图的例子中是切成了16张小图片)</li>
<li>将切片后的图片进行遮盖，和NLP任务中的掩码类似，然后将遮盖的图片展平</li>
<li>将展平后的图片，加上位置编码后送入BEiT 编码器中，得到遮盖后的图片的编码结果</li>
<li>通过掩码图片建模，得到图片遮盖部分的 Visual Token，与原始的图片进行对比</li>
<li>通过解码器还原原始图片</li>
</ol>
</li>
</ol>
<p>==<strong>注</strong>==: 对于图像掩码建模感兴趣的同学可以参考下面三篇文章:</p>
<ol>
<li>BEiT[^7]:  2022年发表在ICLR</li>
<li>MAE[^8]：2022年发表在CVPR</li>
<li>MaskFeat[^9]：2021年发表在CVPR</li>
</ol>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/image-20231229235647408.png" alt="image-20231229235647408" style="zoom:100%;" />

<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/%E6%88%AA%E5%9B%BE-1704093685986.png" alt="截图-1704093685986" style="zoom:100%;" />

<h3 id="2-2-视觉生成-Vision-Generation"><a href="#2-2-视觉生成-Vision-Generation" class="headerlink" title="2.2 视觉生成 Vision Generation"></a>2.2 视觉生成 <br/><span style="font-size:18px;">Vision Generation</span></h3><blockquote>
<p>下图显示了截止到2023年7月，一些具有代表性的<code>文本到图像生成</code>模型，从图中我们可以看到，很多的模型都是由国外的一些公司或者机构提出的，例如：<code>OpenAI</code>，<code>MicroSoft</code>，<code>Nvidia</code>，<code>Meta</code>，<code>Google</code>等，还有一些中国的学校或者机构，例如：<code>清华大学</code></p>
<p>从下图可以看出，国内对于<code>文本到图像生成</code>方面的研究还不是很多，而国外研究相对来说比较多。所以在国内视觉生成方面还是一个非常值得研究的方向。</p>
<p>下面将简单的介绍三种视觉生成方面的研究，分别是：<code>空间可控生成(Spatial Controllable Generation)</code>，<code>基于文本的编辑(Text-based Editing)</code>和<code>文本提示(Text Prompts Following)</code></p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/%E6%88%AA%E5%9B%BE-1703693504301.png" alt="截图-1703693504301" style="zoom:100%;" />

<h4 id="2-2-1-空间可控生成Spatial-Controllable-Generation"><a href="#2-2-1-空间可控生成Spatial-Controllable-Generation" class="headerlink" title="2.2.1 空间可控生成Spatial Controllable Generation"></a>2.2.1 空间可控生成<br/><span style="font-size:16px;">Spatial Controllable Generation</span></h4><blockquote>
<p>Text-to-Image(T2L) 生成通产采用开放式文本供用户描述预期的图像，但是在某些描述中，仅使用文本通常是不能完整，准确的描述出用户预期的图像。</p>
<p><code>空间可控T2L生成(Spatial Controllable Text To Image Generation)</code>探索了一种扩展的T2L模型，使用额外的空间输入来指导图像的生成。</p>
<p>==<strong>注</strong>==: 对于空间可控生成方面感兴趣的同学可以参考下面这篇论文：</p>
<ol>
<li>Reco[^10]：2023年发表在IEEE/CVF</li>
</ol>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/image-20240101231034147.png" alt="image-20240101231034147" style="zoom:100%;" />

<h4 id="2-2-2-基于文本编辑Text-based-Editing"><a href="#2-2-2-基于文本编辑Text-based-Editing" class="headerlink" title="2.2.2 基于文本编辑Text-based Editing"></a>2.2.2 基于文本编辑<br/><span style="font-size:16px;">Text-based Editing</span></h4><blockquote>
<p>T2L编辑基于用户给定的图像和输入的文本描述合成新的图像，不需要完全从头开始生成，和之前的T2L模型存在一些区别，之前的T2L大部分都是从头生成一个图片。</p>
<p>T2L编辑的目标是保留大部分的视觉内容，只修改特定的组件，这可能涉及更改局部对象或整体图像样式以精确匹配用户的意图。</p>
<p>T2L编辑为用户提供了一种工具，可以根据已有的图像生成新的图像，在创建准确遵循人类意图的视觉内容方面发挥着至关重要的作用。</p>
<p>下图介绍了三种T2L编辑方式，分别是：<code>单词替换</code>，<code>添加新短语</code>和<code>注意力权重调整</code>。</p>
<p>==<strong>注</strong>==: 对于基于文本编辑方面感兴趣的同学可以参考下面这篇论文：</p>
<ol>
<li>Prompt- to-prompt image editing with cross-attention control[^11]: 2023年发表在ICLR</li>
</ol>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/%E6%88%AA%E5%9B%BE-1704126148632.png" alt="截图-1704126148632" style="zoom:100%;" />

<blockquote>
<p><code>单词替换</code>，<code>添加新短语</code>和<code>注意力权重调整</code>三种方法的概述：顶部：视觉和文本嵌入使用交叉注意力层进行融合，为每个文本标记生成空间注意力图。底部：我们使用源图像的注意力图来控制生成图像的空间布局和几何结构。这通过仅编辑文本提示来启用各种编辑任务。当在提示中交换单词时，我们注入源图像映射Mt，覆盖目标图像映射M*t，以保留空间布局。其中，在添加新短语的情况下，我们只注入与提示的未更改部分相对应的映射。通过重新加权相应的注意力图来放大或减弱单词的语义效果。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/image-20240102002848562.png" alt="image-20240102002848562" style="zoom:100%;" />

<blockquote>
<p><strong>添加，删除，修改一些单词</strong>：我们的方法提供了各种“提示到提示”编辑功能。用户可以调整形容词的影响程度（左上）、替换图像中的项目（右上）、指定图像的样式（左下）或对生成的图像进行进一步细化（右下）。操作通过扩散模型的交叉注意力机制渗透，而不需要在图像像素空间上进行任何规范。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/image-20240102003100042.png" alt="image-20240102003100042" style="zoom:100%;" />

<blockquote>
<p><strong>注意力权重调整</strong>：通过不同数量的扩散步骤注入注意力。在顶部，我们显示源图像和提示。在每一行中，我们通过替换文本中的单个单词并注入源图像的交叉注意力图来修改图像的内容，该图的范围从0%（左侧）到100%（右侧）的扩散步骤。请注意，一方面，如果没有我们的方法，就不能保证保留任何源图像内容。另一方面，在所有扩散步骤中注入交叉注意力可能会过度约束几何体，导致对文本提示的保真度较低，例如，汽车（第三排）变成了具有完全交叉注意力注入的自行车。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/image-20240102003441391.png" alt="image-20240102003441391" style="zoom:100%;" />

<h4 id="2-2-3-文本提示跟随Text-Prompts-Following"><a href="#2-2-3-文本提示跟随Text-Prompts-Following" class="headerlink" title="2.2.3 文本提示跟随Text Prompts Following"></a>2.2.3 文本提示跟随<br/><span style="font-size:16px;">Text Prompts Following</span></h4><blockquote>
<p>使用图像文本配对进行训练，可以促使T2I模型生成与输入文本条件在语义上相对应的图像。然而，图像生成训练目标并不直接要求生成的图像完全遵循文本提示。有研究表明： T2I模型可能无法完全遵循文本提示，尤其是当图像描述变得复杂时。例如，某些名词短语可能会被省略，属性可能被错误地应用到不正确的对象上，生成的图像可能具有错误的对象数量、关系、风格等。这些限制促使人们致力于改进T2I模型，使其更好地遵循文本提示。</p>
<p>==<strong>注</strong>==: 对于文本提示跟随方面感兴趣的同学可以参考下面这篇论文：</p>
<ol>
<li>DreamBooth[^12]：2023年发表在CVPR</li>
</ol>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/image-20240102144342034.png" alt="image-20240102144342034" style="zoom:100%;" />

<blockquote>
<p>仅凭几张（通常3-5张）关于某个主题（左图）的图片，我们的模型DreamBooth就能在不同的情境（右图）中生成大量该主题的图片，同时利用文本提示的指导。这些结果展现了与环境的自然互动，以及新颖的表达和光照条件的变化，同时保持了对主题关键视觉特征的高度保真。</p>
</blockquote>
<blockquote>
<p>我对于这部分的理解是，用户可以输入3到5张图片，然后输入一个提示，例如上图中演示的样子，然后模型根据用户输入的图片和提示生成对应的图片。上图中输入了几张狗狗的图片，然后输入的提示是<code>&quot;in the Acropolis&quot;</code>，模型就能输出一只在<code>卫城(古希腊的一个地名，现在位于埃及境内)</code>狗狗的图片。</p>
</blockquote>
<h2 id="3-探索性的，开放的多模式基础模型的研究Research-on-exploratory-and-open-multimodal-foundational-models"><a href="#3-探索性的，开放的多模式基础模型的研究Research-on-exploratory-and-open-multimodal-foundational-models" class="headerlink" title="3. 探索性的，开放的多模式基础模型的研究Research on exploratory and open multimodal foundational models"></a>3. 探索性的，开放的多模式基础模型的研究<br/><span style="font-size:22px;">Research on exploratory and open multimodal foundational models</span></h2><h3 id="3-1-统一视觉模型Unified-vision-models"><a href="#3-1-统一视觉模型Unified-vision-models" class="headerlink" title="3.1 统一视觉模型Unified vision models"></a>3.1 统一视觉模型<br/><span style="font-size:18px;">Unified vision models</span></h3><blockquote>
<p> 在“统一视觉模型（Unified Vision Models）”章节中，文章详细探讨了如何构建能够处理多种视觉任务的统一模型架构。本章节的核心内容可以概括为以下几点：</p>
</blockquote>
<blockquote>
<ol>
<li><strong>统一模型架构的提出</strong>：<ul>
<li>提出了一种新的模型设计理念，即通过统一的视觉模型架构来处理不同的视觉任务，如图像分类、对象检测、分割等。</li>
<li>这种架构旨在减少模型的复杂性，提高训练和推理的效率，同时在多个任务上实现更好的性能。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li><strong>视觉和语言的统一</strong>：<ul>
<li>文章讨论了如何将视觉信息和语言信息融合在一个模型中，以便更好地理解和处理视觉内容。</li>
<li>这包括使用共享的表示空间来整合视觉和语言特征，以及如何通过这种融合来提高模型在视觉理解任务上的性能。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="3">
<li><strong>模型训练策略</strong>：<ul>
<li>描述了使用大规模图像数据集进行预训练的方法，以及如何通过微调来适应特定的下游任务。</li>
<li>强调了在预训练阶段使用多样化的数据集，以及在微调阶段使用任务特定的数据集来提高模型的泛化能力。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="4">
<li><strong>功能统一的实现</strong>：<ul>
<li>探讨了如何通过统一的模型架构来实现不同的视觉功能，例如通过共享的编码器和解码器来处理图像分类和分割任务。</li>
<li>讨论了如何通过调整模型的输出层来适应不同的任务需求，例如从图像级到像素级的转换。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="5">
<li><strong>从静态到提示式模型的转变</strong>：<ul>
<li>文章提出了将静态视觉模型转变为能够响应用户指令的提示式模型的方法。</li>
<li>这涉及到模型如何理解和遵循用户的自然语言指令，以及如何将这些指令转化为视觉输出。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<ol start="6">
<li><strong>总结和讨论</strong>：<ul>
<li>总结了统一视觉模型在处理多样化视觉任务中的潜力，以及通过整合视觉和语言信息来提高模型的理解和生成能力。</li>
<li>对未来的研究方向进行了讨论，包括如何进一步提升模型的性能、泛化能力，以及如何更好地理解和生成视觉内容。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<p> 这一章节强调了统一视觉模型在处理多样化视觉任务中的潜力，以及如何通过整合视觉和语言信息来提高模型的理解和生成能力。通过统一模型，研究者们希望能够构建出更加灵活、高效且强大的视觉系统，这些系统能够在多种视觉任务中展现出强大的性能。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/%E6%88%AA%E5%9B%BE-1704180909968.png" alt="截图-1704180909968" style="zoom:100%;" />

<blockquote>
<p> 在自然语言处理（NLP）领域，我们已经见证了构建统一模型的明确趋势，例如GPT-3以及随后的复杂的人-人工智能交互系统ChatGPT，这激发了整个社区和社会对人工智能的兴趣。对于计算机视觉（CV）领域，一个自然的问题是我们是否可以统一所有不同类型的视觉任务，如图像分类、目标检测、分割和视觉问答等，并相应地构建计算机视觉模型与人类之间的交互接口。受到这一愿景的启发，最近已经有许多尝试从不同的角度解决这个问题，包括但不限于（a）使视觉模型成为开放式集合；（b）统一不同的粒度；以及（c）使模型更具提示性。</p>
</blockquote>
<h4 id="3-1-1-从闭集模型到开集模型From-Closed-Set-to-Open-Set-Models"><a href="#3-1-1-从闭集模型到开集模型From-Closed-Set-to-Open-Set-Models" class="headerlink" title="3.1.1 从闭集模型到开集模型From Closed-Set to Open-Set Models"></a>3.1.1 从闭集模型到开集模型<br/><span style="font-size:16px;">From Closed-Set to Open-Set Models</span></h4><blockquote>
<p><strong>问题1</strong>：什么是<code>Closed-Set Models</code>？</p>
<ol>
<li>这些模型是在有限的、预先定义好的类别集合上进行训练的。例如，一个图像分类模型可能只被训练来识别特定的一组类别，如ImageNet数据集中的1000个类别。</li>
<li>封闭集合模型在训练时只考虑这些类别，并且在测试时也仅对这些类别进行预测。</li>
<li>这些模型的一个主要限制是它们无法识别训练数据集中不存在的新类别或未见过的类别。</li>
</ol>
</blockquote>
<blockquote>
<p><strong>问题2</strong>：什么是<code>Open-Set Models</code>？</p>
<ol>
<li>开放集合模型旨在处理超出训练数据集类别范围的新类别。它们不仅能够识别训练过的类别，还能够识别和处理未见过的新类别。</li>
<li>这些模型通常使用一种机制来区分已知类别和未知类别，例如通过概率分布或置信度得分来区分。</li>
<li>开放集合模型在处理现实世界的视觉任务时更为灵活，因为现实世界中存在无数的类别，而不仅仅是训练数据集中的那些。</li>
</ol>
</blockquote>
<blockquote>
<p><strong>问题3</strong>：<code>Closed-Set Models</code> 和 <code>Open-Set Models</code> 有什么相同点和不同点？</p>
<ol>
<li><p><strong>相同点</strong>：</p>
<ul>
<li>无论是封闭集合模型还是开放集合模型，它们都旨在从视觉数据（如图像）中提取有意义的信息，并进行分类或识别。</li>
</ul>
</li>
<li><p><strong>不同点</strong>：</p>
<ul>
<li><strong>类别范围</strong>：封闭集合模型仅针对有限的类别进行训练和测试，而开放集合模型能够处理超出训练数据集的类别。</li>
<li><strong>泛化能力</strong>：开放集合模型通常需要更强的泛化能力，因为它们需要能够识别和处理训练过程中未遇到的新类别。</li>
<li><strong>输出格式</strong>：开放集合模型的输出可能包括一个额外的“未知”类别，而封闭集合模型的输出则限于训练过的类别。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<p><strong>问题4</strong>：如何从<code>Closed-Set Models</code> 转变到 <code>Open-Set Models</code>？</p>
<ol>
<li><strong>数据表示的扩展</strong>：通过引入文本描述或概念嵌入，将图像数据与丰富的语义信息结合起来。例如，使用CLIP模型，可以将图像与描述图像内容的文本描述配对，从而学习到图像和文本之间的映射关系。</li>
<li><strong>训练策略的调整</strong>：采用对比学习等自监督学习方法，使用大量的图像-文本对进行预训练，使模型能够学习到更广泛的视觉特征和语义表示。</li>
<li><strong>模型架构的改进</strong>：设计能够处理开放集合任务的模型架构。例如，可以在模型的输出层添加一个额外的分支，用于输出未知类别的置信度得分。</li>
<li><strong>评估方法的更新</strong>：开发新的评估指标和测试集，以更准确地衡量模型在开放集合设置下的性能。这可能包括设计包含未见过类别的测试集，以及开发能够评估模型对未知类别识别能力的指标。</li>
</ol>
</blockquote>
<h4 id="3-1-2-从特定任务模型到通用模型From-Task-Specific-Models-to-Generic-Models"><a href="#3-1-2-从特定任务模型到通用模型From-Task-Specific-Models-to-Generic-Models" class="headerlink" title="3.1.2 从特定任务模型到通用模型From Task-Specific Models to Generic Models"></a>3.1.2 从特定任务模型到通用模型<br/><span style="font-size:16px;">From Task-Specific Models to Generic Models</span></h4><blockquote>
<p><strong>问题1</strong>：什么是<code>Task-Specific Models</code>？</p>
<ol>
<li><strong>设计目的</strong>：这些模型是为了解决特定的视觉任务而设计的，如图像分类、目标检测、分割等。</li>
<li><strong>训练数据</strong>：它们通常在特定的、有限的数据集上进行训练，例如，一个图像分类模型可能只在ImageNet数据集上进行训练。</li>
<li><strong>性能依赖</strong>：模型的性能高度依赖于训练数据的质量和多样性。如果模型只见过有限的类别，它可能在识别新的、未见过的类别时遇到困难。</li>
<li><strong>架构定制</strong>：为了优化特定任务的性能，这些模型可能会包含为该任务定制的模块或架构。</li>
</ol>
</blockquote>
<blockquote>
<p><strong>问题2</strong>：什么是<code>Generic Models</code>？</p>
<ol>
<li><strong>设计目的</strong>：通用模型旨在处理多种视觉任务，而不是仅限于单一任务。它们的目标是学习一种通用的视觉表示，这种表示可以在多个任务之间迁移。</li>
<li><strong>训练数据</strong>：通用模型通常在大规模、多样化的数据集上进行预训练，以学习丰富的视觉特征和模式。</li>
<li><strong>迁移学习</strong>：通过迁移学习，这些模型可以在预训练的基础上，通过微调来适应特定的视觉任务，从而提高在新任务上的性能。</li>
<li><strong>架构灵活性</strong>：通用模型的架构设计通常更加灵活，可以适应不同的任务需求，而不需要为每个任务重新设计模型。</li>
</ol>
</blockquote>
<blockquote>
<p><strong>问题3</strong>：<code>Task-Specific Models</code> 和 <code>Generic Models</code> 有什么相同点和不同点？</p>
<ol>
<li><p><strong>相同点</strong>：</p>
<ul>
<li>无论是任务特定模型还是通用模型，它们都是计算机视觉模型，旨在从视觉数据中提取有意义的信息。</li>
<li>两者都可以通过深度学习技术进行训练，并且可以利用神经网络架构来学习复杂的视觉特征。</li>
</ul>
</li>
<li><p><strong>不同点</strong>：</p>
<ul>
<li><strong>任务范围</strong>：任务特定模型专注于单一任务，而通用模型旨在处理多种任务。</li>
<li><strong>训练数据</strong>：任务特定模型通常针对特定数据集进行训练，而通用模型则在大规模、多样化的数据集上进行预训练。</li>
<li><strong>迁移学习</strong>：通用模型通过迁移学习可以适应新任务，而任务特定模型则需要为每个新任务重新训练。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<p><strong>问题4</strong>：如何从<code>Task-Specific Models</code> 转变到 <code>Generic Models</code>？</p>
<ul>
<li><strong>预训练</strong>：在大规模数据集上进行预训练，以学习通用的视觉特征。这可以通过使用无监督或自监督学习方法来完成，如对比学习、掩模自编码等。</li>
<li><strong>微调</strong>：在预训练的基础上，对模型进行微调，以适应特定的视觉任务。这通常涉及到在特定任务的数据集上进行额外的训练。</li>
<li><strong>模型设计</strong>：设计能够适应多种任务的模型架构。例如，使用统一的编码器-解码器结构，或者通过模块化设计来整合不同的视觉任务。</li>
<li><strong>损失函数和训练策略</strong>：开发新的损失函数和训练策略，以促进模型在多个任务上的学习。这可能包括多任务学习、元学习等方法。</li>
</ul>
</blockquote>
<h4 id="3-1-3-从静态模型到可提示模型From-Static-to-Promptable-Models"><a href="#3-1-3-从静态模型到可提示模型From-Static-to-Promptable-Models" class="headerlink" title="3.1.3 从静态模型到可提示模型From Static to Promptable Models"></a>3.1.3 从静态模型到可提示模型<br/><span style="font-size:16px;">From Static to Promptable Models</span></h4><blockquote>
<p><strong>问题1</strong>：什么是<code>Static Models</code>？</p>
<ol>
<li><strong>定义</strong>：静态模型是指那些在特定任务上进行训练，并且在测试时仅执行这些任务的模型。它们通常不涉及与用户的交互，而是直接对输入数据进行处理并输出结果。</li>
<li><strong>交互性</strong>：静态模型的交互性有限，用户通常只能提供数据输入，而不能直接影响模型的决策过程。</li>
<li><strong>灵活性</strong>：由于静态模型针对特定任务进行优化，它们在处理未在训练数据中见过的新情况时可能表现不佳。</li>
</ol>
</blockquote>
<blockquote>
<p><strong>问题2</strong>：什么是<code>Promptable Models</code>？</p>
<ol>
<li><strong>定义</strong>：可提示模型允许用户通过自然语言提示来引导模型的行为。这些模型通常结合了视觉处理能力和语言理解能力，能够根据用户的指令执行多样化的任务。</li>
<li><strong>交互性</strong>：可提示模型提供了一种更自然、更直观的交互方式，用户可以通过文本提示来指导模型的输出。</li>
<li><strong>灵活性</strong>：这些模型通常更灵活，因为它们可以根据用户的提示适应不同的任务需求，即使这些任务在训练时并未直接见过。</li>
</ol>
</blockquote>
<blockquote>
<p><strong>问题3</strong>：<code>Static Models</code> 和 <code>Promptable Models</code> 有什么相同点和不同点？</p>
<ol>
<li><strong>相同点</strong>：无论是静态模型还是可提示模型，它们都是基于深度学习的计算机视觉模型，旨在处理视觉数据并生成有意义的输出。</li>
<li><strong>不同点</strong>：<ul>
<li><strong>交互方式</strong>：静态模型通常不提供交互界面，而可提示模型允许用户通过文本提示与模型进行交互。</li>
<li><strong>灵活性</strong>：可提示模型在任务执行上的灵活性更高，因为它们可以根据用户的指令适应不同的任务需求。</li>
<li><strong>输出控制</strong>：静态模型的输出通常是固定的，而可提示模型的输出可以根据用户的提示进行调整。</li>
</ul>
</li>
</ol>
</blockquote>
<blockquote>
<p><strong>问题4</strong>：如何从<code>Static Models</code> 转变到 <code>Promptable Models</code>？</p>
<ol>
<li><strong>集成语言模型</strong>：将视觉模型与大型语言模型（如GPT系列）结合，使模型能够理解和响应自然语言指令。这通常涉及到将视觉模型的输出与语言模型的输入结合起来。</li>
<li><strong>训练策略</strong>：开发新的训练策略，使模型能够在视觉任务中学习和利用自然语言提示。这可能包括对比学习、强化学习等方法，以提高模型对提示的敏感性和响应性。</li>
<li><strong>模型架构</strong>：设计能够处理文本提示的模型架构。这可能包括添加文本编码器来处理用户输入的提示，以及调整视觉模型的输出层以适应提示引导的任务。</li>
<li><strong>用户界面</strong>：构建用户界面，允许用户通过文本提示与模型进行交互。这可能包括聊天界面、图形用户界面（GUI）或其他用户友好的交互方式。</li>
</ol>
</blockquote>
<h3 id="3-2-大语言模型加持的多模态大模型Large-Multimodal-Models-Training-with-LLM"><a href="#3-2-大语言模型加持的多模态大模型Large-Multimodal-Models-Training-with-LLM" class="headerlink" title="3.2 大语言模型加持的多模态大模型Large Multimodal Models: Training with LLM"></a>3.2 大语言模型加持的多模态大模型<br/><span style="font-size:18px;">Large Multimodal Models: Training with LLM</span></h3><h4 id="3-2-1-背景Background"><a href="#3-2-1-背景Background" class="headerlink" title="3.2.1 背景Background"></a>3.2.1 背景<br/><span style="font-size:16px;">Background</span></h4><blockquote>
<p>大型语言模型（LLMs）结合进行训练的研究背景的总结：</p>
<ol>
<li><strong>多模态生成模型</strong>：介绍了多模态生成模型的基本概念，这些模型能够将图像作为输入，输出文本序列。这些模型通常包含一个图像编码器来提取视觉特征，以及一个语言模型来生成文本序列。</li>
<li><strong>模型架构</strong>：描述了这些模型的典型架构，包括图像编码器和语言模型，以及它们之间的连接模块。图像编码器和语言模型可以是从头开始训练的，也可以是从预训练模型初始化的。</li>
<li><strong>训练目标</strong>：阐述了模型的训练目标，通常采用自回归损失函数在输出文本序列上进行训练。在Transformer架构中，图像标记可以相互关注，当前文本标记可以关注所有图像标记以及之前的文本标记。</li>
<li><strong>案例研究</strong>：提供了一些著名的大型多模态模型（如GIT和BLIP2）作为案例研究，展示了如何在不同的模型架构中实现上述网络结构，同时保持相同的自回归训练目标。</li>
<li><strong>多模态上下文学习</strong>：特别强调了多模态上下文学习（Multimodal In-Context-Learning）这一新兴特性，这是Flamingo模型的一个显著特点。这种能力使得模型能够通过少量示例进行零样本任务迁移，解决未见过的复杂问题。</li>
<li><strong>OpenAI Multimodal GPT-4</strong>：提到了OpenAI发布的GPT-4模型，它展示了在视觉理解和推理方面的强能力，尽管模型的具体细节尚未公开。</li>
<li><strong>研究差距</strong>：指出了当前多模态模型与OpenAI Multimodal GPT-4之间存在的差距，特别是在如何执行指令跟随和对齐人类意图的研究方面。</li>
</ol>
<p>这一章节为读者提供了大型多模态模型的背景知识，特别是它们如何与大型语言模型结合进行训练，以及这些模型在视觉理解和生成任务中的最新进展。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/david-deng-01/images/blog/%E6%88%AA%E5%9B%BE-1704273389055.png" alt="截图-1704273389055" style="zoom:100%;" />

<blockquote>
<p>OpenAI 提出的多模态GPT-4，视觉样例来自OpenAI[^13]</p>
</blockquote>
<h2 id="4-总结和展望"><a href="#4-总结和展望" class="headerlink" title="4. 总结和展望"></a>4. 总结和展望</h2><h3 id="3-3-总结Summary-and-Conclusions"><a href="#3-3-总结Summary-and-Conclusions" class="headerlink" title="3.3 总结Summary and Conclusions"></a>3.3 总结<br/><span style="font-size:18px;">Summary and Conclusions</span></h3><blockquote>
<ol>
<li><strong>特定目的的多模态基础模型</strong>：这些模型针对计算机视觉社区中的多样化问题进行了广泛的研究。为了为引入通用视觉助手打下全面基础，讨论了许多研讨会论文，涵盖了在预训练时代的问题。主要范式是在大量与问题相关的数据上进行预训练，然后以零或少次的方式转移到相同问题类型的现实世界场景中。具体来说，章节讨论了两个一般主题：<ul>
<li>在第2章视觉理解中：各个多模态基础模型已经发展到可以在图像、区域、像素级别分析视觉内容，语言增强视觉模型是最近在野外视觉理解任务中取得成功的流行家族。 </li>
<li>在第3章视觉生成中：文本到图像生成模型为图像合成奠定了基础，已经成功地扩展到允许用户以更细粒度的方式控制和自定义图像。大量与问题相关的数据的可用性在使这些多模态基础模型成为可能方面发挥了关键作用。</li>
</ul>
</li>
<li><strong>通用助手</strong>：作者回顾了最近出现的文献，关于构建通用助手，这些助手通常具有统一的网络架构、统一的输入输出数据格式以及便于与人类轻松互动的通用接口。受到大型语言模型（LLM）如ChatGPT/GPT-4作为广泛语言任务的通用助手的启发，计算机视觉研究人员探索了针对视觉任务的对应解决方案。根据LLM在方法论中的利用方式，现有工作可以分为三个主题：<ul>
<li>第4章统一视觉模型中：借鉴LLM中统一建模的精神，构建不同级别和跨不同任务的统一视觉模型。</li>
<li>第5章与LLM一起训练中：从预训练的LLM开始，将视觉数据连接到LLM进行端到端训练。</li>
<li>第6章与LLM链接中：通过冻结LLM，现有的视觉专家可以通过提示工程LLM来完成特定视觉任务。</li>
</ul>
</li>
</ol>
</blockquote>
<h3 id="3-3-展望Research-Trends"><a href="#3-3-展望Research-Trends" class="headerlink" title="3.3 展望Research Trends"></a>3.3 展望<br/><span style="font-size:18px;">Research Trends</span></h3><blockquote>
<p>构建通用目的AI代理的研究方向和趋势的主要内容：</p>
<ol>
<li><strong>通用代理与多模态性</strong>：<ul>
<li>作者强调了构建一个单一的通用代理，它能够通过融合多种渠道（如语言、视觉、语音和行动）与世界互动，类似于人类。在这种背景下，多模态基础模型的概念变得不那么突出，而是成为代理感知和合成视觉信号的关键组成部分。</li>
<li>提出了构建一个能够像人类一样通过多种渠道（如语言、视觉、语音和动作）与世界互动的单一通用代理的目标。</li>
<li>多模态基础模型被视为代理感知和合成视觉信号的关键组成部分，而不是单独的模态。例如，Gato（Reed等人，2022）和PaLM-E（Driess等人，2023）展示了如何使用单一的模型权重执行广泛的语言、多模态和控制任务，其中视觉感知是理解环境的关键组成部分。</li>
</ul>
</li>
<li><strong>与人类意图的对齐</strong>：<ul>
<li>AI对齐研究关注于引导AI系统朝着人类的预期目标、价值观或道德准则发展。</li>
<li>尽管语言在表达人类意图方面表现出了通用性，但视觉提示（如关键点、边界框和素描）在视觉理解和生成任务中可以更精确和方便地表示人类意图。</li>
<li>构建具备多模态人机交互界面的基础模型是解锁新用例的关键，其中人类意图最好通过视觉表示。</li>
</ul>
</li>
<li><strong>规划、记忆和工具使用</strong>：<ul>
<li>提到了LLM驱动的自主代理系统，其中LLM作为代理的大脑，并辅以规划、记忆和工具使用等关键组件。</li>
<li><strong>规划</strong>：为了在现实世界场景中完成复杂任务，代理需要能够将大型任务分解为更小、更易管理的子目标。理想情况下，AI代理应具备自我改进能力，通过自我评估和内省来学习从错误中吸取教训，并改进后续行动的方法，最终实现更好的结果。</li>
<li><strong>记忆</strong>：短期记忆可以通过上下文学习（或提示工程）实现，而长期记忆则需要模型能够快速检索跨会话的外部知识。</li>
<li><strong>工具使用</strong>：代理需要学习利用外部API来获取基础模型权重中缺失的知识。在处理视觉模态的多种场景时，需要新的处理能力。</li>
</ul>
</li>
<li><strong>研究趋势</strong>：<ul>
<li>作者强调了多模态基础模型领域的快速发展，并指出新的方向和方法不断涌现。尽管有许多重要的研究主题没有在本文中讨论，主要是因为研究创新每天都在更新，但作者对多模态基础模型的未来持乐观态度。</li>
<li>作者相信，通过遵循LLM的道路，可以预见到各个领域的令人兴奋的研究创新和想法在不久的将来成为现实，并且将计算机视觉与更广泛的AI社区联系起来，构建通用目的的AI代理将显著推进人类日常生活。</li>
</ul>
</li>
</ol>
<p>文章强调了多模态基础模型在构建通用AI代理中的重要性，并提出了未来研究的方向，包括提升模型的规划、记忆和工具使用能力，以及如何更好地与人类意图对齐。</p>
</blockquote>
<h2 id="参考文件或链接"><a href="#参考文件或链接" class="headerlink" title="参考文件或链接"></a>参考文件或链接</h2><p>[^1]:<a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/2309.10020">Multimodal Foundation Models: From Specialists to General-Purpose Assistants (arxiv.org)</a><br>[^2]:<a target="_blank" rel="noopener external nofollow noreferrer" href="https://mp.weixin.qq.com/s/wC4q-GRt8YY4u8-vOl6uMg">多模态大模型最全综述来了！ (qq.com)</a></p>
<p>[^3]:<a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/2108.07258">On the Opportunities and Risks of Foundation Models (arxiv.org)</a></p>
<p>[^4]:<a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/openai/CLIP">openai/CLIP - GitHub</a><br>[^5]:<a target="_blank" rel="noopener external nofollow noreferrer" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Exploring_Simple_Siamese_Representation_Learning_CVPR_2021_paper.pdf">Exploring Simple Siamese Representation Learning (thecvf.com)</a><br>[^6]:<a target="_blank" rel="noopener external nofollow noreferrer" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.pdf">Emerging Properties in Self-Supervised Vision Transformers (thecvf.com)</a><br>[^7]: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/pdf/2106.08254.pdf">BEIT: BERT Pre-Training of Image Transformers(arxiv.org)</a><br>[^8]: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf">Masked Autoencoders Are Scalable Vision Learners (thecvf.com)</a><br>[^9]: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wei_Masked_Feature_Prediction_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.pdf">Masked Feature Prediction for Self-Supervised Visual Pre-Training (thecvf.com)</a><br>[^10]: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf">ReCo: Region-Controlled Text-to-Image Generation (thecvf.com)</a><br>[^11]: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/pdf/2208.01626.pdf">Prompt- to-prompt image editing with cross-attention control (arxiv.org)</a><br>[^12]: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf">DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation (thecvf.com)</a><br>[^13]: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/pdf/2303.08774.pdf">GPT-4 Technical Report (arxiv.org)</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://david-deng.cn">David</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://david-deng.cn/2024/01/04/2024-01-05%20%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/">https://david-deng.cn/2024/01/04/2024-01-05 多模态模型综述/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://david-deng.cn" target="_blank">David 的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a></div><div class="post_share"><div class="social-share" data-image="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-4.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/WeChatPay.jpg" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/WeChatPay.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/Alipay.jpg" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/Alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/12/30/%E6%A0%A1%E5%9B%AD%E7%BD%91%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%95%E8%84%9A%E6%9C%AC/" title="校园网自动登录脚本"><img class="cover" src="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-8.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">校园网自动登录脚本</div></div></a></div><div class="next-post pull-right"><a href="/2024/03/18/other-leetcode-%E3%80%902024-03-18-LeetCode-%E5%88%B7%E9%A2%98-400%E9%81%93%E6%89%93%E5%8D%A1%E3%80%91/" title="LeetCode刷题400道打卡"><img class="cover" src="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-6.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LeetCode刷题400道打卡</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/10/25/1%202023-10-25%20%E7%BB%84%E4%BC%9A%E5%88%86%E4%BA%AB/" title="【2023-10-25 组会分享】大语言模型综述"><img class="cover" src="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-8.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-25</div><div class="title">【2023-10-25 组会分享】大语言模型综述</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/loading.gif'" alt="avatar"/></div><div class="author-info__name">David</div><div class="author-info__description">Welcome to David's Blog</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">28</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">27</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/david-deng-01"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/david-deng-01" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/David_0925" rel="external nofollow noreferrer" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=635647792&amp;website=www.oicqzone.com" rel="external nofollow noreferrer" target="_blank" title="QQ"><i class="fab fa-qq"></i></a><a class="social-icon" href="mailto:david-deng-0925@qq.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Multimodal-Foundation-Models-From-Specialists-to-General-Purpose-Assistants-1-%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%EF%BC%9A%E4%BB%8E%E4%B8%93%E5%AE%B6%E5%88%B0%E9%80%9A%E7%94%A8%E5%8A%A9%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">Multimodal Foundation Models: From Specialists to General-Purpose Assistants[^1]多模态基础模型：从专家到通用助理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-%E6%96%87%E7%AB%A0%E7%AE%80%E4%BB%8BPaper-information"><span class="toc-number">1.1.</span> <span class="toc-text">0. 文章简介Paper information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BC%95%E8%A8%80Introduction"><span class="toc-number">1.2.</span> <span class="toc-text">1. 引言Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%EF%BC%9F-What-are-Multimodal-Foundation-Models"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.1 什么是多模态基础模型？ What are Multimodal Foundation Models?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%88%90%E7%86%9F%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6Well-established-research-of-multimodal-foundation-models"><span class="toc-number">1.3.</span> <span class="toc-text">2. 成熟的多模态基础模型研究Well-established research of multimodal foundation models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%A7%86%E8%A7%89%E7%90%86%E8%A7%A3-Visual-understanding"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1 视觉理解 Visual understanding</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-1-%E6%A0%87%E7%AD%BE%E7%9B%91%E7%9D%A3Label-supervision"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">2.1.1 标签监督Label supervision</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-2-%E8%AF%AD%E8%A8%80%E7%9B%91%E7%9D%A3Language-Supervision"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">2.1.2 语言监督Language Supervision</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-3-%E4%BB%85%E4%BD%BF%E7%94%A8%E5%9B%BE%E7%89%87%E7%9A%84%E8%87%AA%E7%9B%91%E7%9D%A3Image-only-Self-supervision"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">2.1.3 仅使用图片的自监督Image-only Self-supervision</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%A7%86%E8%A7%89%E7%94%9F%E6%88%90-Vision-Generation"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2 视觉生成 Vision Generation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-%E7%A9%BA%E9%97%B4%E5%8F%AF%E6%8E%A7%E7%94%9F%E6%88%90Spatial-Controllable-Generation"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">2.2.1 空间可控生成Spatial Controllable Generation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-%E5%9F%BA%E4%BA%8E%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91Text-based-Editing"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">2.2.2 基于文本编辑Text-based Editing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-%E6%96%87%E6%9C%AC%E6%8F%90%E7%A4%BA%E8%B7%9F%E9%9A%8FText-Prompts-Following"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">2.2.3 文本提示跟随Text Prompts Following</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%8E%A2%E7%B4%A2%E6%80%A7%E7%9A%84%EF%BC%8C%E5%BC%80%E6%94%BE%E7%9A%84%E5%A4%9A%E6%A8%A1%E5%BC%8F%E5%9F%BA%E7%A1%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%A0%94%E7%A9%B6Research-on-exploratory-and-open-multimodal-foundational-models"><span class="toc-number">1.4.</span> <span class="toc-text">3. 探索性的，开放的多模式基础模型的研究Research on exploratory and open multimodal foundational models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E7%BB%9F%E4%B8%80%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8BUnified-vision-models"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1 统一视觉模型Unified vision models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-%E4%BB%8E%E9%97%AD%E9%9B%86%E6%A8%A1%E5%9E%8B%E5%88%B0%E5%BC%80%E9%9B%86%E6%A8%A1%E5%9E%8BFrom-Closed-Set-to-Open-Set-Models"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">3.1.1 从闭集模型到开集模型From Closed-Set to Open-Set Models</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-%E4%BB%8E%E7%89%B9%E5%AE%9A%E4%BB%BB%E5%8A%A1%E6%A8%A1%E5%9E%8B%E5%88%B0%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8BFrom-Task-Specific-Models-to-Generic-Models"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">3.1.2 从特定任务模型到通用模型From Task-Specific Models to Generic Models</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-%E4%BB%8E%E9%9D%99%E6%80%81%E6%A8%A1%E5%9E%8B%E5%88%B0%E5%8F%AF%E6%8F%90%E7%A4%BA%E6%A8%A1%E5%9E%8BFrom-Static-to-Promptable-Models"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">3.1.3 从静态模型到可提示模型From Static to Promptable Models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8A%A0%E6%8C%81%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8BLarge-Multimodal-Models-Training-with-LLM"><span class="toc-number">1.4.2.</span> <span class="toc-text">3.2 大语言模型加持的多模态大模型Large Multimodal Models: Training with LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E8%83%8C%E6%99%AFBackground"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">3.2.1 背景Background</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%80%BB%E7%BB%93%E5%92%8C%E5%B1%95%E6%9C%9B"><span class="toc-number">1.5.</span> <span class="toc-text">4. 总结和展望</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E6%80%BB%E7%BB%93Summary-and-Conclusions"><span class="toc-number">1.5.1.</span> <span class="toc-text">3.3 总结Summary and Conclusions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%B1%95%E6%9C%9BResearch-Trends"><span class="toc-number">1.5.2.</span> <span class="toc-text">3.3 展望Research Trends</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E4%BB%B6%E6%88%96%E9%93%BE%E6%8E%A5"><span class="toc-number">1.6.</span> <span class="toc-text">参考文件或链接</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/03/18/other-leetcode-%E3%80%902024-03-18-LeetCode-%E5%88%B7%E9%A2%98-400%E9%81%93%E6%89%93%E5%8D%A1%E3%80%91/" title="LeetCode刷题400道打卡"><img src="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-6.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode刷题400道打卡"/></a><div class="content"><a class="title" href="/2024/03/18/other-leetcode-%E3%80%902024-03-18-LeetCode-%E5%88%B7%E9%A2%98-400%E9%81%93%E6%89%93%E5%8D%A1%E3%80%91/" title="LeetCode刷题400道打卡">LeetCode刷题400道打卡</a><time datetime="2024-03-17T16:00:00.000Z" title="发表于 2024-03-18 00:00:00">2024-03-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/01/04/2024-01-05%20%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/" title="多模态基础模型：从专家到通用助理"><img src="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="多模态基础模型：从专家到通用助理"/></a><div class="content"><a class="title" href="/2024/01/04/2024-01-05%20%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E7%BB%BC%E8%BF%B0/" title="多模态基础模型：从专家到通用助理">多模态基础模型：从专家到通用助理</a><time datetime="2024-01-04T14:42:08.000Z" title="发表于 2024-01-04 22:42:08">2024-01-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/12/30/%E6%A0%A1%E5%9B%AD%E7%BD%91%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%95%E8%84%9A%E6%9C%AC/" title="校园网自动登录脚本"><img src="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-8.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="校园网自动登录脚本"/></a><div class="content"><a class="title" href="/2023/12/30/%E6%A0%A1%E5%9B%AD%E7%BD%91%E8%87%AA%E5%8A%A8%E7%99%BB%E5%BD%95%E8%84%9A%E6%9C%AC/" title="校园网自动登录脚本">校园网自动登录脚本</a><time datetime="2023-12-30T09:21:30.000Z" title="发表于 2023-12-30 17:21:30">2023-12-30</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/11/24/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BD%E5%88%BA%E8%AF%86%E5%88%AB%E5%9F%BA%E7%BA%BF%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0/" title="多模态讽刺识别基线模型复现"><img src="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-3.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="多模态讽刺识别基线模型复现"/></a><div class="content"><a class="title" href="/2023/11/24/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BD%E5%88%BA%E8%AF%86%E5%88%AB%E5%9F%BA%E7%BA%BF%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0/" title="多模态讽刺识别基线模型复现">多模态讽刺识别基线模型复现</a><time datetime="2023-11-24T08:51:18.000Z" title="发表于 2023-11-24 16:51:18">2023-11-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/11/24/%E5%AE%9D%E5%A1%94%E9%9D%A2%E6%9D%BF%E5%A4%B1%E6%95%88/" title="宝塔面板失效"><img src="https://jsd.012700.xyz/gh/jerryc127/CDN/img/material-1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="宝塔面板失效"/></a><div class="content"><a class="title" href="/2023/11/24/%E5%AE%9D%E5%A1%94%E9%9D%A2%E6%9D%BF%E5%A4%B1%E6%95%88/" title="宝塔面板失效">宝塔面板失效</a><time datetime="2023-11-24T08:19:27.000Z" title="发表于 2023-11-24 16:19:27">2023-11-24</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By David</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a href="http://www.beian.gov.cn/portal/registerSystemInfo" rel="external nofollow noreferrer" target="_blank"> <img style="vertical-align:middle; width:20px; " src="https://cdn.jsdelivr.net/gh/David-deng-01/images/blog/icp.png"> 赣公网安备36082302000135号</a> <a href="https://beian.miit.gov.cn/" rel="external nofollow noreferrer" id="beian"  target="_blank">赣ICP备2023013705号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>